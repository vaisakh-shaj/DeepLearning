{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Long Term Short Term Memory(LSTMs)\n",
    "------------\n",
    "\n",
    "Task is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data.\n",
    "\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data Creation - training valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "99993848 american individualist anarchism benjamin tucker in one eight tw\n",
      "1000 anarchism originated as a term of abuse first used against early\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_text = ' '.join(str(e) for e in train_text)\n",
    "valid_text = ' '.join(str(e) for e in valid_text)\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing As Per Our Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' o', 'or', 'ri', 'ig', 'gi', 'in', 'na', 'at', 'te', 'ed', 'd ', ' a', 'as', 's ', ' a', 'a ', ' t', 'te', 'er', 'rm', 'm ', ' o', 'of', 'f ', ' a', 'ab', 'bu', 'us', 'se', 'e ', ' f', 'fi', 'ir', 'rs', 'st', 't ', ' u', 'us', 'se', 'ed', 'd ', ' a', 'ag', 'ga', 'ai', 'in', 'ns', 'st', 't ', ' e', 'ea', 'ar', 'rl', 'ly', 'y ', ' w', 'wo', 'or', 'rk', 'ki', 'in', 'ng', 'g ', ' c', 'cl', 'la', 'as', 'ss', 's ', ' r', 'ra', 'ad', 'di', 'ic', 'ca', 'al', 'ls', 's ', ' i', 'in', 'nc', 'cl', 'lu', 'ud', 'di', 'in', 'ng', 'g ', ' t', 'th', 'he', 'e ', ' d', 'di', 'ig', 'gg', 'ge', 'er', 'rs', 's ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' e', 'en', 'ng', 'gl', 'li', 'is', 'sh', 'h ', ' r', 're', 'ev', 'vo', 'ol', 'lu', 'ut', 'ti', 'io', 'on', 'n ', ' a', 'an', 'nd', 'd ', ' t', 'th', 'he', 'e ', ' s', 'sa', 'an', 'ns', 's ', ' c', 'cu', 'ul', 'lo', 'ot', 'tt', 'te', 'es', 's ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' f', 'fr', 're', 'en', 'nc', 'ch', 'h ', ' r', 're', 'ev', 'vo', 'ol', 'lu', 'ut', 'ti', 'io', 'on', 'n ', ' w', 'wh', 'hi', 'il', 'ls', 'st', 't ', ' t', 'th', 'he', 'e ', ' t', 'te', 'er', 'rm', 'm ', ' i', 'is', 's ', ' s', 'st', 'ti', 'il', 'll', 'l ', ' u', 'us', 'se', 'ed', 'd ', ' i', 'in', 'n ', ' a', 'a ', ' p', 'pe', 'ej', 'jo', 'or', 'ra', 'at', 'ti', 'iv', 've', 'e ', ' w', 'wa', 'ay', 'y ', ' t', 'to', 'o ', ' d', 'de', 'es', 'sc', 'cr', 'ri', 'ib', 'be', 'e ', ' a', 'an', 'ny', 'y ', ' a', 'ac', 'ct', 't ', ' t', 'th', 'ha', 'at', 't ', ' u', 'us', 'se', 'ed', 'd ', ' v', 'vi', 'io', 'ol', 'le', 'en', 'nt', 't ', ' m', 'me', 'ea', 'an', 'ns', 's ', ' t', 'to', 'o ', ' d', 'de', 'es', 'st', 'tr', 'ro', 'oy', 'y ', ' t', 'th', 'he', 'e ', ' o', 'or', 'rg', 'ga', 'an', 'ni', 'iz', 'za', 'at', 'ti', 'io', 'on', 'n ', ' o', 'of', 'f ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' i', 'it', 't ', ' h', 'ha', 'as', 's ', ' a', 'al', 'ls', 'so', 'o ', ' b', 'be', 'ee', 'en', 'n ', ' t', 'ta', 'ak', 'ke', 'en', 'n ', ' u', 'up', 'p ', ' a', 'as', 's ', ' a', 'a ', ' p', 'po', 'os', 'si', 'it', 'ti', 'iv', 've', 'e ', ' l', 'la', 'ab', 'be', 'el', 'l ', ' b', 'by', 'y ', ' s', 'se', 'el', 'lf', 'f ', ' d', 'de', 'ef', 'fi', 'in', 'ne', 'ed', 'd ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'ts', 's ', ' t', 'th', 'he', 'e ', ' w', 'wo', 'or', 'rd', 'd ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' i', 'is', 's ', ' d', 'de', 'er', 'ri', 'iv', 've', 'ed', 'd ', ' f', 'fr', 'ro', 'om', 'm ', ' t', 'th', 'he', 'e ', ' g', 'gr', 're', 'ee', 'ek', 'k ', ' w', 'wi', 'it', 'th', 'ho', 'ou', 'ut', 't ', ' a', 'ar', 'rc', 'ch', 'ho', 'on', 'ns', 's ', ' r', 'ru', 'ul', 'le', 'er', 'r ', ' c', 'ch', 'hi', 'ie', 'ef', 'f ', ' k', 'ki', 'in', 'ng', 'g ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' a', 'as', 's ', ' a', 'a ', ' p', 'po', 'ol', 'li', 'it', 'ti', 'ic', 'ca', 'al', 'l ', ' p', 'ph', 'hi', 'il', 'lo', 'os', 'so', 'op', 'ph', 'hy', 'y ', ' i', 'is', 's ', ' t', 'th', 'he', 'e ', ' b', 'be', 'el', 'li', 'ie', 'ef', 'f ', ' t', 'th', 'ha', 'at', 't ', ' r', 'ru', 'ul', 'le', 'er', 'rs', 's ', ' a', 'ar', 're', 'e ', ' u', 'un', 'nn', 'ne', 'ec', 'ce', 'es', 'ss', 'sa', 'ar', 'ry', 'y ', ' a', 'an', 'nd', 'd ', ' s', 'sh', 'ho', 'ou', 'ul', 'ld', 'd ', ' b', 'be', 'e ', ' a', 'ab', 'bo', 'ol', 'li', 'is', 'sh', 'he', 'ed', 'd ', ' a', 'al', 'lt', 'th', 'ho', 'ou', 'ug', 'gh', 'h ', ' t', 'th', 'he', 'er', 're', 'e ', ' a', 'ar', 're', 'e ', ' d', 'di', 'if', 'ff', 'fe', 'er', 'ri', 'in', 'ng', 'g ', ' i', 'in', 'nt', 'te', 'er', 'rp', 'pr', 're', 'et', 'ta', 'at', 'ti', 'io', 'on', 'ns', 's ', ' o', 'of', 'f ', ' w', 'wh', 'ha', 'at', 't ', ' t', 'th', 'hi', 'is', 's ', ' m', 'me', 'ea', 'an', 'ns', 's ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' a', 'al', 'ls', 'so', 'o ', ' r', 're', 'ef', 'fe', 'er', 'rs', 's ', ' t', 'to', 'o ', ' r', 're', 'el', 'la', 'at', 'te', 'ed', 'd ', ' s', 'so', 'oc', 'ci', 'ia', 'al', 'l ', ' m', 'mo', 'ov', 've', 'em', 'me', 'en', 'nt', 'ts', 's ', ' t', 'th', 'ha', 'at', 't ', ' a', 'ad', 'dv', 'vo', 'oc', 'ca', 'at', 'te', 'e ', ' t', 'th', 'he', 'e ', ' e', 'el', 'li', 'im', 'mi', 'in', 'na', 'at', 'ti', 'io', 'on', 'n ', ' o', 'of', 'f ', ' a', 'au', 'ut', 'th', 'ho', 'or', 'ri', 'it', 'ta', 'ar', 'ri', 'ia', 'an', 'n ', ' i', 'in', 'ns', 'st', 'ti', 'it', 'tu', 'ut', 'ti', 'io', 'on', 'ns', 's ', ' p', 'pa', 'ar', 'rt', 'ti', 'ic', 'cu', 'ul', 'la', 'ar', 'rl', 'ly', 'y ', ' t', 'th', 'he', 'e ', ' s', 'st', 'ta', 'at', 'te', 'e ', ' t', 'th', 'he', 'e ', ' w', 'wo', 'or', 'rd', 'd ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hy', 'y ', ' a', 'as', 's ', ' m', 'mo', 'os', 'st', 't ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'ts', 's ', ' u', 'us', 'se', 'e ', ' i', 'it', 't ', ' d', 'do', 'oe', 'es', 's ', ' n', 'no', 'ot', 't ', ' i', 'im', 'mp', 'pl', 'ly', 'y ', ' c', 'ch', 'ha', 'ao', 'os', 's ', ' n', 'ni', 'ih', 'hi', 'il', 'li', 'is', 'sm', 'm ', ' o', 'or', 'r ', ' a', 'an', 'no', 'om', 'mi', 'ie', 'e ', ' b', 'bu', 'ut', 't ', ' r', 'ra', 'at', 'th', 'he', 'er', 'r ', ' a', 'a ', ' h', 'ha', 'ar', 'rm', 'mo', 'on', 'ni', 'io', 'ou', 'us', 's ', ' a', 'an', 'nt', 'ti', 'i ', ' a', 'au', 'ut', 'th', 'ho', 'or', 'ri', 'it', 'ta', 'ar', 'ri', 'ia', 'an', 'n ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' i', 'in', 'n ', ' p', 'pl', 'la', 'ac', 'ce', 'e ', ' o', 'of', 'f ', ' w', 'wh', 'ha', 'at', 't ', ' a', 'ar', 're', 'e ', ' r', 're', 'eg', 'ga', 'ar', 'rd', 'de', 'ed', 'd ', ' a', 'as', 's ', ' a', 'au', 'ut', 'th', 'ho', 'or', 'ri', 'it', 'ta', 'ar', 'ri', 'ia', 'an', 'n ', ' p', 'po', 'ol', 'li', 'it', 'ti', 'ic', 'ca', 'al', 'l ', ' s', 'st', 'tr', 'ru', 'uc', 'ct', 'tu', 'ur', 're', 'es', 's ', ' a', 'an', 'nd', 'd ', ' c', 'co', 'oe', 'er', 'rc', 'ci', 'iv', 've', 'e ', ' e', 'ec', 'co', 'on', 'no', 'om', 'mi', 'ic', 'c ', ' i', 'in', 'ns', 'st', 'ti', 'it', 'tu', 'ut', 'ti', 'io', 'on', 'ns', 's ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'ts', 's ', ' a', 'ad', 'dv', 'vo', 'oc', 'ca', 'at', 'te', 'e ', ' s', 'so', 'oc', 'ci', 'ia', 'al', 'l ', ' r', 're', 'el', 'la', 'at', 'ti', 'io', 'on', 'ns', 's ', ' b', 'ba', 'as', 'se', 'ed', 'd ', ' u', 'up', 'po', 'on', 'n ', ' v', 'vo', 'ol', 'lu', 'un', 'nt', 'ta', 'ar', 'ry', 'y ', ' a', 'as', 'ss', 'so', 'oc', 'ci', 'ia', 'at', 'ti', 'io', 'on', 'n ', ' o', 'of', 'f ', ' a', 'au', 'ut', 'to', 'on', 'no', 'om', 'mo', 'ou', 'us', 's ', ' i', 'in', 'nd', 'di', 'iv', 'vi', 'id', 'du', 'ua', 'al', 'ls', 's ', ' m', 'mu', 'ut', 'tu', 'ua', 'al', 'l ', ' a', 'ai', 'id', 'd ', ' a', 'an', 'nd', 'd ', ' s', 'se', 'el', 'lf', 'f ', ' g', 'go', 'ov', 've', 'er', 'rn', 'na', 'an', 'nc', 'ce', 'e ', ' w', 'wh', 'hi', 'il', 'le', 'e ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' i', 'is', 's ', ' m', 'mo', 'os', 'st', 't ', ' e', 'ea', 'as', 'si', 'il', 'ly', 'y ', ' d', 'de', 'ef', 'fi', 'in', 'ne', 'ed', 'd ', ' b', 'by', 'y ', ' w', 'wh', 'ha', 'at', 't ', ' i', 'it', 't ', ' i', 'is', 's ', ' a', 'ag', 'ga', 'ai', 'in', 'ns', 'st', 't ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'ts', 's ', ' a', 'al', 'ls', 'so', 'o ', ' o', 'of', 'ff', 'fe', 'er', 'r ', ' p', 'po', 'os', 'si', 'it', 'ti', 'iv', 've', 'e ', ' v', 'vi', 'is', 'si', 'io', 'on', 'ns', 's ', ' o', 'of', 'f ', ' w', 'wh', 'ha', 'at', 't ', ' t', 'th', 'he', 'ey', 'y ', ' b', 'be', 'el', 'li', 'ie', 'ev', 've', 'e ', ' t', 'to', 'o ', ' b', 'be', 'e ', ' a', 'a ', ' t', 'tr', 'ru', 'ul', 'ly', 'y ', ' f', 'fr', 're', 'ee', 'e ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' h', 'ho', 'ow', 'we', 'ev', 've', 'er', 'r ', ' i', 'id', 'de', 'ea', 'as', 's ', ' a', 'ab', 'bo', 'ou', 'ut', 't ', ' h', 'ho', 'ow', 'w ', ' a', 'an', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' m', 'mi', 'ig', 'gh', 'ht', 't ', ' w', 'wo', 'or', 'rk', 'k ', ' v', 'va', 'ar', 'ry', 'y ', ' c', 'co', 'on', 'ns', 'si', 'id', 'de', 'er', 'ra', 'ab', 'bl', 'ly', 'y ', ' e', 'es', 'sp', 'pe', 'ec', 'ci', 'ia', 'al', 'll', 'ly', 'y ', ' w', 'wi', 'it', 'th', 'h ', ' r', 're', 'es', 'sp', 'pe', 'ec', 'ct', 't ', ' t', 'to', 'o ', ' e', 'ec', 'co', 'on', 'no', 'om', 'mi', 'ic', 'cs', 's ', ' t', 'th', 'he', 'er', 're', 'e ', ' i', 'is', 's ', ' a', 'al', 'ls', 'so', 'o ', ' d', 'di', 'is', 'sa', 'ag', 'gr', 're', 'ee', 'em', 'me', 'en', 'nt', 't ', ' a', 'ab', 'bo', 'ou', 'ut', 't ', ' h', 'ho', 'ow', 'w ', ' a', 'a ', ' f', 'fr', 're', 'ee', 'e ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' m', 'mi', 'ig', 'gh', 'ht', 't ', ' b', 'be', 'e ', ' b', 'br', 'ro', 'ou', 'ug', 'gh', 'ht', 't ', ' a', 'ab', 'bo', 'ou', 'ut', 't ', ' o', 'or', 'ri', 'ig', 'gi', 'in', 'ns', 's ', ' a', 'an', 'nd', 'd ', ' p', 'pr', 're', 'ed', 'de', 'ec', 'ce', 'es', 'ss', 'so', 'or', 'rs', 's ', ' k', 'kr', 'ro', 'op', 'po', 'ot', 'tk', 'ki', 'in', 'n ', ' a', 'an', 'nd', 'd ', ' o', 'ot', 'th', 'he', 'er', 'rs', 's ', ' a', 'ar', 'rg', 'gu', 'ue', 'e ', ' t', 'th', 'ha', 'at', 't ', ' b', 'be', 'ef', 'fo', 'or', 're', 'e ', ' r', 're', 'ec', 'co', 'or', 'rd', 'de', 'ed', 'd ', ' h', 'hi', 'is', 'st', 'to', 'or', 'ry', 'y ', ' h', 'hu', 'um', 'ma', 'an', 'n ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' w', 'wa', 'as', 's ', ' o', 'or', 'rg', 'ga', 'an', 'ni', 'iz', 'ze', 'ed', 'd ', ' o', 'on', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' p', 'pr', 'ri', 'in', 'nc', 'ci', 'ip', 'pl', 'le', 'es', 's ', ' m', 'mo', 'os', 'st', 't ', ' a', 'an', 'nt', 'th', 'hr', 'ro', 'op', 'po', 'ol', 'lo', 'og', 'gi', 'is', 'st', 'ts', 's ', ' f', 'fo', 'ol', 'll', 'lo', 'ow', 'w ', ' k', 'kr', 'ro', 'op', 'po', 'ot', 'tk', 'ki', 'in', 'n ', ' a', 'an', 'nd', 'd ', ' e', 'en', 'ng', 'ge', 'el', 'ls', 's ', ' i', 'in', 'n ', ' b', 'be', 'el', 'li', 'ie', 'ev', 'vi', 'in', 'ng', 'g ', ' t', 'th', 'ha', 'at', 't ', ' h', 'hu', 'un', 'nt', 'te', 'er', 'r ', ' g', 'ga', 'at', 'th', 'he', 'er', 're', 'er', 'r ', ' b', 'ba', 'an', 'nd', 'ds', 's ', ' w', 'we', 'er', 're', 'e ', ' e', 'eg', 'ga', 'al', 'li', 'it', 'ta', 'ar', 'ri', 'ia', 'an', 'n ', ' a', 'an', 'nd', 'd ', ' l', 'la', 'ac', 'ck', 'ke', 'ed', 'd ', ' d', 'di', 'iv', 'vi', 'is', 'si', 'io', 'on', 'n ', ' o', 'of', 'f ', ' l', 'la', 'ab', 'bo', 'ou', 'ur', 'r ', ' a', 'ac', 'cc', 'cu', 'um', 'mu', 'ul', 'la', 'at', 'te', 'ed', 'd ', ' w', 'we', 'ea', 'al', 'lt', 'th', 'h ', ' o', 'or', 'r ', ' d', 'de', 'ec', 'cr', 're', 'ee', 'ed', 'd ', ' l', 'la', 'aw', 'w ', ' a', 'an', 'nd', 'd ', ' h', 'ha', 'ad', 'd ', ' e', 'eq', 'qu', 'ua', 'al', 'l ', ' a', 'ac', 'cc', 'ce', 'es', 'ss', 's ', ' t', 'to', 'o ', ' r', 're', 'es', 'so', 'ou', 'ur', 'rc', 'ce', 'es', 's ', ' w', 'wi', 'il', 'll', 'li', 'ia', 'am', 'm ', ' g', 'go', 'od', 'dw', 'wi', 'in', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'ts', 's ', ' i', 'in', 'nc', 'cl', 'lu', 'ud', 'di', 'in', 'ng', 'g ', ' t', 'th', 'he', 'e ', ' t', 'th', 'he', 'e ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hy', 'y ', ' o', 'or', 'rg', 'ga', 'an', 'ni', 'is', 'sa', 'at', 'ti', 'io', 'on', 'n ', ' a', 'an', 'nd', 'd ', ' r', 'ro', 'ot', 'th', 'hb', 'ba', 'ar', 'rd', 'd ', ' f', 'fi', 'in', 'nd', 'd ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' a', 'at', 'tt', 'ti', 'it', 'tu', 'ud', 'de', 'es', 's ', ' i', 'in', 'n ', ' t', 'ta', 'ao', 'oi', 'is', 'sm', 'm ', ' f', 'fr', 'ro', 'om', 'm ', ' a', 'an', 'nc', 'ci', 'ie', 'en', 'nt', 't ', ' c', 'ch', 'hi', 'in', 'na', 'a ', ' k', 'kr', 'ro', 'op', 'po', 'ot', 'tk', 'ki', 'in', 'n ', ' f', 'fo', 'ou', 'un', 'nd', 'd ', ' s', 'si', 'im', 'mi', 'il', 'la', 'ar', 'r ', ' i', 'id', 'de', 'ea', 'as', 's ', ' i', 'in', 'n ', ' s', 'st', 'to', 'oi', 'ic', 'c ', ' z', 'ze', 'en', 'no', 'o ', ' o', 'of', 'f ', ' c', 'ci', 'it', 'ti', 'iu', 'um', 'm ', ' a', 'ac', 'cc', 'co', 'or', 'rd', 'di', 'in', 'ng', 'g ', ' t', 'to', 'o ', ' k', 'kr', 'ro', 'op', 'po', 'ot', 'tk', 'ki', 'in', 'n ', ' z', 'ze', 'en', 'no', 'o ', ' r', 're', 'ep', 'pu', 'ud', 'di', 'ia', 'at', 'te', 'ed', 'd ', ' t', 'th', 'he', 'e ', ' o', 'om', 'mn', 'ni', 'ip', 'po', 'ot', 'te', 'en', 'nc', 'ce', 'e ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' s', 'st', 'ta', 'at', 'te', 'e ', ' i', 'it', 'ts', 's ', ' i', 'in', 'nt', 'te', 'er', 'rv', 've', 'en', 'nt', 'ti', 'io', 'on', 'n ', ' a', 'an', 'nd', 'd ', ' r', 're', 'eg', 'gi', 'im', 'me', 'en', 'nt', 'ta', 'at', 'ti', 'io', 'on', 'n ', ' a', 'an', 'nd', 'd ', ' p', 'pr', 'ro', 'oc', 'cl', 'la', 'ai', 'im', 'me', 'ed', 'd ', ' t', 'th', 'he', 'e ', ' s', 'so', 'ov', 've', 'er', 're', 'ei', 'ig', 'gn', 'nt', 'ty', 'y ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' m', 'mo', 'or', 'ra', 'al', 'l ', ' l', 'la', 'aw', 'w ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' i', 'in', 'nd', 'di', 'iv', 'vi', 'id', 'du', 'ua', 'al', 'l ', ' t', 'th', 'he', 'e ', ' a', 'an', 'na', 'ab', 'ba', 'ap', 'pt', 'ti', 'is', 'st', 'ts', 's ', ' o', 'of', 'f ', ' o', 'on', 'ne', 'e ', ' s', 'si', 'ix', 'x ', ' t', 'th', 'h ', ' c', 'ce', 'en', 'nt', 'tu', 'ur', 'ry', 'y ', ' e', 'eu', 'ur', 'ro', 'op', 'pe', 'e ', ' a', 'ar', 're', 'e ', ' s', 'so', 'om', 'me', 'et', 'ti', 'im', 'me', 'es', 's ', ' c', 'co', 'on', 'ns', 'si', 'id', 'de', 'er', 're', 'ed', 'd ', ' t', 'to', 'o ', ' b', 'be', 'e ', ' r', 're', 'el', 'li', 'ig', 'gi', 'io', 'ou', 'us', 's ', ' f', 'fo', 'or', 're', 'er', 'ru', 'un', 'nn', 'ne', 'er', 'rs', 's ', ' o', 'of', 'f ', ' m', 'mo', 'od', 'de', 'er', 'rn', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' b', 'be', 'er', 'rt', 'tr', 'ra', 'an', 'nd', 'd ', ' r', 'ru', 'us', 'ss', 'se', 'el', 'll', 'l ', ' i', 'in', 'n ', ' h', 'hi', 'is', 's ', ' h', 'hi', 'is', 'st', 'to', 'or', 'ry', 'y ', ' o', 'of', 'f ', ' w', 'we', 'es', 'st', 'te', 'er', 'rn', 'n ', ' p', 'ph', 'hi', 'il', 'lo', 'os', 'so', 'op', 'ph', 'hy', 'y ', ' w', 'wr', 'ri', 'it', 'te', 'es', 's ', ' t', 'th', 'ha', 'at', 't ', ' t', 'th', 'he', 'e ', ' a', 'an', 'na', 'ab', 'ba', 'ap', 'pt', 'ti', 'is', 'st', 'ts', 's ', ' r', 're', 'ep', 'pu', 'ud', 'di', 'ia', 'at', 'te', 'ed', 'd ', ' a', 'al', 'll', 'l ', ' l', 'la', 'aw', 'w ', ' s', 'si', 'in', 'nc', 'ce', 'e ', ' t', 'th', 'he', 'ey', 'y ', ' h', 'he', 'el', 'ld', 'd ', ' t', 'th', 'ha', 'at', 't ', ' t', 'th', 'he', 'e ', ' g', 'go', 'oo', 'od', 'd ', ' m', 'ma', 'an', 'n ', ' w', 'wi', 'il', 'll', 'l ', ' b', 'be', 'e ', ' g', 'gu', 'ui', 'id', 'de', 'ed', 'd ', ' a', 'at', 't ', ' e', 'ev', 've', 'er', 'ry', 'y ', ' m', 'mo', 'om', 'me', 'en', 'nt', 't ', ' b', 'by', 'y ', ' t', 'th', 'he', 'e ', ' h', 'ho', 'ol', 'ly', 'y ', ' s', 'sp', 'pi', 'ir', 'ri', 'it', 't ', ' f', 'fr', 'ro', 'om', 'm ', ' t', 'th', 'hi', 'is', 's ', ' p', 'pr', 're', 'em', 'mi', 'is', 'se', 'e ', ' t', 'th', 'he', 'ey', 'y ', ' a', 'ar', 'rr', 'ri', 'iv', 've', 'e ', ' a', 'at', 't ', ' c', 'co', 'om', 'mm', 'mu', 'un', 'ni', 'is', 'sm', 'm ', ' t', 'th', 'he', 'e ', ' d', 'di', 'ig', 'gg', 'ge', 'er', 'rs', 's ', ' o', 'or', 'r ', ' t', 'tr', 'ru', 'ue', 'e ', ' l', 'le', 'ev', 've', 'el', 'll', 'le', 'er', 'rs', 's ', ' w', 'we', 'er', 're', 'e ', ' a', 'an', 'n ', ' e', 'ea', 'ar', 'rl', 'ly', 'y ', ' c', 'co', 'om', 'mm', 'mu', 'un', 'ni', 'is', 'st', 'ti', 'ic', 'c ', ' m', 'mo', 'ov', 've', 'em', 'me', 'en', 'nt', 't ', ' d', 'du', 'ur', 'ri', 'in', 'ng', 'g ', ' t', 'th', 'he', 'e ', ' t', 'ti', 'im', 'me', 'e ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' e', 'en', 'ng', 'gl', 'li', 'is', 'sh', 'h ', ' c', 'ci', 'iv', 'vi', 'il', 'l ', ' w', 'wa', 'ar', 'r ', ' a', 'an', 'nd', 'd ', ' a', 'ar', 're', 'e ', ' c', 'co', 'on', 'ns', 'si', 'id', 'de', 'er', 're', 'ed', 'd ', ' b', 'by', 'y ', ' s', 'so', 'om', 'me', 'e ', ' a', 'as', 's ', ' f', 'fo', 'or', 're', 'er', 'ru', 'un', 'nn', 'ne', 'er', 'rs', 's ', ' o', 'of', 'f ', ' m', 'mo', 'od', 'de', 'er', 'rn', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' i', 'in', 'n ', ' t', 'th', 'he', 'e ', ' m', 'mo', 'od', 'de', 'er', 'rn', 'n ', ' e', 'er', 'ra', 'a ', ' t', 'th', 'he', 'e ', ' f', 'fi', 'ir', 'rs', 'st', 't ', ' t', 'to', 'o ', ' u', 'us', 'se', 'e ', ' t', 'th', 'he', 'e ', ' t', 'te', 'er', 'rm', 'm ', ' t', 'to', 'o ', ' m', 'me', 'ea', 'an', 'n ', ' s', 'so', 'om', 'me', 'et', 'th', 'hi', 'in', 'ng', 'g ', ' o', 'ot', 'th', 'he', 'er', 'r ', ' t', 'th', 'ha', 'an', 'n ', ' c', 'ch', 'ha', 'ao', 'os', 's ', ' w', 'wa', 'as', 's ', ' l', 'lo', 'ou', 'ui', 'is', 's ', ' a', 'ar', 'rm', 'ma', 'an', 'nd', 'd ', ' b', 'ba', 'ar', 'ro', 'on', 'n ', ' d', 'de', 'e ', ' l', 'la', 'ah', 'ho', 'on', 'nt', 'ta', 'an', 'n ', ' i', 'in', 'n ', ' h', 'hi', 'is', 's ', ' n', 'no', 'ou', 'uv', 've', 'ea', 'au', 'ux', 'x ', ' v', 'vo', 'oy', 'ya', 'ag', 'ge', 'es', 's ', ' d', 'da', 'an', 'ns', 's ', ' l', 'l ', ' a', 'am', 'm ', ' r', 'ri', 'iq', 'qu', 'ue', 'e ', ' s', 'se', 'ep', 'pt', 'te', 'en', 'nt', 'tr', 'ri', 'io', 'on', 'na', 'al', 'le', 'e ', ' o', 'on', 'ne', 'e ', ' s', 'se', 'ev', 've', 'en', 'n ', ' z', 'ze', 'er', 'ro', 'o ', ' t', 'th', 'hr', 're', 'ee', 'e ', ' w', 'wh', 'he', 'er', 're', 'e ', ' h', 'he', 'e ', ' d', 'de', 'es', 'sc', 'cr', 'ri', 'ib', 'be', 'ed', 'd ', ' t', 'th', 'he', 'e ', ' i', 'in', 'nd', 'di', 'ig', 'ge', 'en', 'no', 'ou', 'us', 's ', ' a', 'am', 'me', 'er', 'ri', 'ic', 'ca', 'an', 'n ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' w', 'wh', 'hi', 'ic', 'ch', 'h ', ' h', 'ha', 'ad', 'd ', ' n', 'no', 'o ', ' s', 'st', 'ta', 'at', 'te', 'e ', ' l', 'la', 'aw', 'ws', 's ', ' p', 'pr', 'ri', 'is', 'so', 'on', 'ns', 's ', ' p', 'pr', 'ri', 'ie', 'es', 'st', 'ts', 's ', ' o', 'or', 'r ', ' p', 'pr', 'ri', 'iv', 'va', 'at', 'te', 'e ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' a', 'as', 's ', ' b', 'be', 'ei', 'in', 'ng', 'g ', ' i', 'in', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hy', 'y ', ' r', 'ru', 'us', 'ss', 'se', 'el', 'll', 'l ', ' m', 'me', 'ea', 'an', 'ns', 's ', ' a', 'a ', ' l', 'li', 'ib', 'be', 'er', 'rt', 'ta', 'ar', 'ri', 'ia', 'an', 'n ', ' a', 'an', 'nd', 'd ', ' l', 'le', 'ea', 'ad', 'de', 'er', 'r ', ' i', 'in', 'n ', ' t', 'th', 'he', 'e ', ' a', 'am', 'me', 'er', 'ri', 'ic', 'ca', 'an', 'n ', ' i', 'in', 'nd', 'di', 'ia', 'an', 'n ', ' m', 'mo', 'ov', 've', 'em', 'me', 'en', 'nt', 't ', ' h', 'ha', 'as', 's ', ' r', 're', 'ep', 'pe', 'ea', 'at', 'te', 'ed', 'dl', 'ly', 'y ', ' s', 'st', 'ta', 'at', 'te', 'ed', 'd ', ' t', 'th', 'ha', 'at', 't ', ' h', 'he', 'e ', ' i', 'is', 's ', ' a', 'an', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' a', 'an', 'nd', 'd ', ' s', 'so', 'o ', ' a', 'ar', 're', 'e ', ' a', 'al', 'll', 'l ', ' h', 'hi', 'is', 's ', ' a', 'an', 'nc', 'ce', 'es', 'st', 'to', 'or', 'rs', 's ', ' i', 'in', 'n ', ' o', 'on', 'ne', 'e ', ' s', 'se', 'ev', 've', 'en', 'n ', ' n', 'ni', 'in', 'ne', 'e ', ' t', 'th', 'hr', 're', 'ee', 'e ', ' i', 'in', 'n ', ' t', 'th', 'he', 'e ', ' t', 'th', 'hi', 'ic', 'ck', 'k ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' f', 'fr', 're', 'en', 'nc', 'ch', 'h ', ' r', 're', 'ev', 'vo', 'ol', 'lu', 'ut', 'ti', 'io', 'on', 'n ', ' w', 'wi', 'il', 'll', 'li', 'ia', 'am', 'm ', ' g', 'go', 'od', 'dw', 'wi', 'in', 'n ', ' p', 'pu', 'ub', 'bl', 'li', 'is', 'sh', 'he', 'ed', 'd ', ' a', 'an', 'n ', ' e', 'en', 'nq', 'qu', 'ui', 'ir', 'ry', 'y ', ' c', 'co', 'on', 'nc', 'ce', 'er', 'rn', 'ni', 'in', 'ng', 'g ', ' p', 'po', 'ol', 'li', 'it', 'ti', 'ic', 'ca', 'al', 'l ', ' j', 'ju', 'us', 'st', 'ti', 'ic', 'ce', 'e ', ' a', 'al', 'lt', 'th', 'ho', 'ou', 'ug', 'gh', 'h ', ' g', 'go', 'od', 'dw', 'wi', 'in', 'n ', ' d', 'di', 'id', 'd ', ' n', 'no', 'ot', 't ', ' u', 'us', 'se', 'e ', ' t', 'th', 'he', 'e ', ' w', 'wo', 'or', 'rd', 'd ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' m', 'ma', 'an', 'ny', 'y ', ' l', 'la', 'at', 'te', 'er', 'r ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'ts', 's ', ' h', 'ha', 'av', 've', 'e ', ' r', 're', 'eg', 'ga', 'ar', 'rd', 'de', 'ed', 'd ', ' t', 'th', 'hi', 'is', 's ', ' b', 'bo', 'oo', 'ok', 'k ', ' a', 'as', 's ', ' t', 'th', 'he', 'e ', ' f', 'fi', 'ir', 'rs', 'st', 't ', ' m', 'ma', 'aj', 'jo', 'or', 'r ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' t', 'te', 'ex', 'xt', 't ', ' a', 'an', 'nd', 'd ', ' g', 'go', 'od', 'dw', 'wi', 'in', 'n ', ' a', 'as', 's ', ' t', 'th', 'he', 'e ', ' f', 'fo', 'ou', 'un', 'nd', 'de', 'er', 'r ', ' o', 'of', 'f ', ' p', 'ph', 'hi', 'il', 'lo', 'os', 'so', 'op', 'ph', 'hi', 'ic', 'ca', 'al', 'l ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' b', 'bu', 'ut', 't ', ' a', 'at', 't ', ' t', 'th', 'hi', 'is', 's ', ' p', 'po', 'oi', 'in', 'nt', 't ', ' n', 'no', 'o ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' m', 'mo', 'ov', 've', 'em', 'me', 'en', 'nt', 't ', ' y', 'ye', 'et', 't ', ' e', 'ex', 'xi', 'is', 'st', 'te', 'ed', 'd ', ' a', 'an', 'nd', 'd ', ' t', 'th', 'he', 'e ', ' t', 'te', 'er', 'rm', 'm ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'te', 'e ', ' w', 'wa', 'as', 's ', ' k', 'kn', 'no', 'ow', 'wn', 'n ', ' m', 'ma', 'ai', 'in', 'nl', 'ly', 'y ', ' a', 'as', 's ', ' a', 'an', 'n ', ' i', 'in', 'ns', 'su', 'ul', 'lt', 't ', ' h', 'hu', 'ur', 'rl', 'le', 'ed', 'd ', ' b', 'by', 'y ', ' t', 'th', 'he', 'e ', ' b', 'bo', 'ou', 'ur', 'rg', 'ge', 'eo', 'oi', 'is', 's ', ' g', 'gi', 'ir', 'ro', 'on', 'nd', 'di', 'in', 'ns', 's ', ' a', 'at', 't ', ' m', 'mo', 'or', 're', 'e ', ' r', 'ra', 'ad', 'di', 'ic', 'ca', 'al', 'l ', ' e', 'el', 'le', 'em', 'me', 'en', 'nt', 'ts', 's ', ' i', 'in', 'n ', ' t', 'th', 'he', 'e ', ' f', 'fr', 're', 'en', 'nc', 'ch', 'h ', ' r', 're', 'ev', 'vo', 'ol', 'lu', 'ut', 'ti', 'io', 'on', 'n ', ' t', 'th', 'he', 'e ', ' f', 'fi', 'ir', 'rs', 'st', 't ', ' s', 'se', 'el', 'lf', 'f ', ' l', 'la', 'ab', 'be', 'el', 'll', 'le', 'ed', 'd ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' p', 'pi', 'ie', 'er', 'rr', 're', 'e ', ' j', 'jo', 'os', 'se', 'ep', 'ph', 'h ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' i', 'it', 't ', ' i', 'is', 's ', ' c', 'co', 'om', 'mm', 'mo', 'on', 'nl', 'ly', 'y ', ' h', 'he', 'el', 'ld', 'd ', ' t', 'th', 'ha', 'at', 't ', ' i', 'it', 't ', ' w', 'wa', 'as', 'sn', 'n ', ' t', 't ', ' u', 'un', 'nt', 'ti', 'il', 'l ', ' p', 'pi', 'ie', 'er', 'rr', 're', 'e ', ' j', 'jo', 'os', 'se', 'ep', 'ph', 'h ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' p', 'pu', 'ub', 'bl', 'li', 'is', 'sh', 'he', 'ed', 'd ', ' w', 'wh', 'ha', 'at', 't ', ' i', 'is', 's ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' i', 'in', 'n ', ' o', 'on', 'ne', 'e ', ' e', 'ei', 'ig', 'gh', 'ht', 't ', ' f', 'fo', 'ou', 'ur', 'r ', ' z', 'ze', 'er', 'ro', 'o ', ' t', 'th', 'ha', 'at', 't ', ' t', 'th', 'he', 'e ', ' t', 'te', 'er', 'rm', 'm ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' w', 'wa', 'as', 's ', ' a', 'ad', 'do', 'op', 'pt', 'te', 'ed', 'd ', ' a', 'as', 's ', ' a', 'a ', ' s', 'se', 'el', 'lf', 'f ', ' d', 'de', 'es', 'sc', 'cr', 'ri', 'ip', 'pt', 'ti', 'io', 'on', 'n ', ' i', 'it', 't ', ' i', 'is', 's ', ' f', 'fo', 'or', 'r ', ' t', 'th', 'hi', 'is', 's ', ' r', 're', 'ea', 'as', 'so', 'on', 'n ', ' t', 'th', 'ha', 'at', 't ', ' s', 'so', 'om', 'me', 'e ', ' c', 'cl', 'la', 'ai', 'im', 'm ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' a', 'as', 's ', ' t', 'th', 'he', 'e ', ' f', 'fo', 'ou', 'un', 'nd', 'de', 'er', 'r ', ' o', 'of', 'f ', ' m', 'mo', 'od', 'de', 'er', 'rn', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' t', 'th', 'he', 'eo', 'or', 'ry', 'y ', ' i', 'in', 'n ', ' w', 'wh', 'ha', 'at', 't ', ' i', 'is', 's ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' a', 'an', 'ns', 'sw', 'we', 'er', 'rs', 's ', ' w', 'wi', 'it', 'th', 'h ', ' t', 'th', 'he', 'e ', ' f', 'fa', 'am', 'mo', 'ou', 'us', 's ', ' a', 'ac', 'cc', 'cu', 'us', 'sa', 'at', 'ti', 'io', 'on', 'n ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' i', 'is', 's ', ' t', 'th', 'he', 'ef', 'ft', 't ', ' i', 'in', 'n ', ' t', 'th', 'hi', 'is', 's ', ' w', 'wo', 'or', 'rk', 'k ', ' h', 'he', 'e ', ' o', 'op', 'pp', 'po', 'os', 'se', 'ed', 'd ', ' t', 'th', 'he', 'e ', ' i', 'in', 'ns', 'st', 'ti', 'it', 'tu', 'ut', 'ti', 'io', 'on', 'n ', ' o', 'of', 'f ', ' d', 'de', 'ec', 'cr', 're', 'ee', 'ed', 'd ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' p', 'pr', 'ro', 'op', 'pr', 'ri', 'i ', ' t', 't ', ' w', 'wh', 'he', 'er', 're', 'e ', ' o', 'ow', 'wn', 'ne', 'er', 'rs', 's ', ' h', 'ha', 'av', 've', 'e ', ' c', 'co', 'om', 'mp', 'pl', 'le', 'et', 'te', 'e ', ' r', 'ri', 'ig', 'gh', 'ht', 'ts', 's ', ' t', 'to', 'o ', ' u', 'us', 'se', 'e ', ' a', 'an', 'nd', 'd ', ' a', 'ab', 'bu', 'us', 'se', 'e ', ' t', 'th', 'he', 'ei', 'ir', 'r ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' a', 'as', 's ', ' t', 'th', 'he', 'ey', 'y ', ' w', 'wi', 'is', 'sh', 'h ', ' s', 'su', 'uc', 'ch', 'h ', ' a', 'as', 's ', ' e', 'ex', 'xp', 'pl', 'lo', 'oi', 'it', 'ti', 'in', 'ng', 'g ', ' w', 'wo', 'or', 'rk', 'ke', 'er', 'rs', 's ', ' f', 'fo', 'or', 'r ', ' p', 'pr', 'ro', 'of', 'fi', 'it', 't ', ' i', 'in', 'n ', ' i', 'it', 'ts', 's ', ' p', 'pl', 'la', 'ac', 'ce', 'e ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' s', 'su', 'up', 'pp', 'po', 'or', 'rt', 'te', 'ed', 'd ', ' w', 'wh', 'ha', 'at', 't ', ' h', 'he', 'e ', ' c', 'ca', 'al', 'll', 'le', 'ed', 'd ', ' p', 'po', 'os', 'ss', 'se', 'es', 'ss', 'si', 'io', 'on', 'n ', ' i', 'in', 'nd', 'di', 'iv', 'vi', 'id', 'du', 'ua', 'al', 'ls', 's ', ' c', 'ca', 'an', 'n ', ' h', 'ha', 'av', 've', 'e ', ' l', 'li', 'im', 'mi', 'it', 'te', 'ed', 'd ', ' r', 'ri', 'ig', 'gh', 'ht', 'ts', 's ', ' t', 'to', 'o ', ' u', 'us', 'se', 'e ', ' r', 're', 'es', 'so', 'ou', 'ur', 'rc', 'ce', 'es', 's ', ' c', 'ca', 'ap', 'pi', 'it', 'ta', 'al', 'l ', ' a', 'an', 'nd', 'd ', ' g', 'go', 'oo', 'od', 'ds', 's ', ' i', 'in', 'n ', ' a', 'ac', 'cc', 'co', 'or', 'rd', 'da', 'an', 'nc', 'ce', 'e ', ' w', 'wi', 'it', 'th', 'h ', ' p', 'pr', 'ri', 'in', 'nc', 'ci', 'ip', 'pl', 'le', 'es', 's ', ' o', 'of', 'f ', ' e', 'eq', 'qu', 'ua', 'al', 'li', 'it', 'ty', 'y ', ' a', 'an', 'nd', 'd ', ' j', 'ju', 'us', 'st', 'ti', 'ic', 'ce', 'e ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' s', 's ', ' v', 'vi', 'is', 'si', 'io', 'on', 'n ', ' o', 'of', 'f ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hy', 'y ', ' w', 'wh', 'hi', 'ic', 'ch', 'h ', ' h', 'he', 'e ', ' c', 'ca', 'al', 'll', 'le', 'ed', 'd ', ' m', 'mu', 'ut', 'tu', 'ua', 'al', 'li', 'is', 'sm', 'm ', ' m', 'mu', 'ut', 'tu', 'ue', 'el', 'll', 'li', 'is', 'sm', 'me', 'e ', ' i', 'in', 'nv', 'vo', 'ol', 'lv', 've', 'ed', 'd ', ' a', 'an', 'n ', ' e', 'ex', 'xc', 'ch', 'ha', 'an', 'ng', 'ge', 'e ', ' e', 'ec', 'co', 'on', 'no', 'om', 'my', 'y ', ' w', 'wh', 'he', 'er', 're', 'e ', ' i', 'in', 'nd', 'di', 'iv', 'vi', 'id', 'du', 'ua', 'al', 'ls', 's ', ' a', 'an', 'nd', 'd ', ' g', 'gr', 'ro', 'ou', 'up', 'ps', 's ', ' c', 'co', 'ou', 'ul', 'ld', 'd ', ' t', 'tr', 'ra', 'ad', 'de', 'e ', ' t', 'th', 'he', 'e ', ' p', 'pr', 'ro', 'od', 'du', 'uc', 'ct', 'ts', 's ', ' o', 'of', 'f ', ' t', 'th', 'he', 'ei', 'ir', 'r ', ' l', 'la', 'ab', 'bo', 'or', 'r ', ' u', 'us', 'si', 'in', 'ng', 'g ', ' l', 'la', 'ab', 'bo', 'or', 'r ', ' n', 'no', 'ot', 'te', 'es', 's ', ' w', 'wh', 'hi', 'ic', 'ch', 'h ', ' r', 're', 'ep', 'pr', 're', 'es', 'se', 'en', 'nt', 'te', 'ed', 'd ', ' t', 'th', 'he', 'e ', ' a', 'am', 'mo', 'ou', 'un', 'nt', 't ', ' o', 'of', 'f ', ' w', 'wo', 'or', 'rk', 'ki', 'in', 'ng', 'g ', ' t', 'ti', 'im', 'me', 'e ', ' i', 'in', 'nv', 'vo', 'ol', 'lv', 've', 'ed', 'd ', ' i', 'in', 'n ', ' p', 'pr', 'ro', 'od', 'du', 'uc', 'ct', 'ti', 'io', 'on', 'n ', ' t', 'th', 'hi', 'is', 's ', ' w', 'wo', 'ou', 'ul', 'ld', 'd ', ' e', 'en', 'ns', 'su', 'ur', 're', 'e ', ' t', 'th', 'ha', 'at', 't ', ' n', 'no', 'o ', ' o', 'on', 'ne', 'e ', ' w', 'wo', 'ou', 'ul', 'ld', 'd ', ' p', 'pr', 'ro', 'of', 'fi', 'it', 't ', ' f', 'fr', 'ro', 'om', 'm ', ' t', 'th', 'he', 'e ', ' l', 'la', 'ab', 'bo', 'or', 'r ', ' o', 'of', 'f ', ' o', 'ot', 'th', 'he', 'er', 'rs', 's ', ' w', 'wo', 'or', 'rk', 'ke', 'er', 'rs', 's ', ' c', 'co', 'ou', 'ul', 'ld', 'd ', ' f', 'fr', 're', 'ee', 'el', 'ly', 'y ', ' j', 'jo', 'oi', 'in', 'n ', ' t', 'to', 'og', 'ge', 'et', 'th', 'he', 'er', 'r ', ' i', 'in', 'n ', ' c', 'co', 'o ', ' o', 'op', 'pe', 'er', 'ra', 'at', 'ti', 'iv', 've', 'e ', ' w', 'wo', 'or', 'rk', 'ks', 'sh', 'ho', 'op', 'ps', 's ', ' a', 'an', 'n ', ' i', 'in', 'nt', 'te', 'er', 're', 'es', 'st', 't ', ' f', 'fr', 're', 'ee', 'e ', ' b', 'ba', 'an', 'nk', 'k ', ' w', 'wo', 'ou', 'ul', 'ld', 'd ', ' b', 'be', 'e ', ' s', 'se', 'et', 't ', ' u', 'up', 'p ', ' t', 'to', 'o ', ' p', 'pr', 'ro', 'ov', 'vi', 'id', 'de', 'e ', ' e', 'ev', 've', 'er', 'ry', 'yo', 'on', 'ne', 'e ', ' w', 'wi', 'it', 'th', 'h ', ' a', 'ac', 'cc', 'ce', 'es', 'ss', 's ', ' t', 'to', 'o ', ' t', 'th', 'he', 'e ', ' m', 'me', 'ea', 'an', 'ns', 's ', ' o', 'of', 'f ', ' p', 'pr', 'ro', 'od', 'du', 'uc', 'ct', 'ti', 'io', 'on', 'n ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' s', 's ', ' i', 'id', 'de', 'ea', 'as', 's ', ' w', 'we', 'er', 're', 'e ', ' i', 'in', 'nf', 'fl', 'lu', 'ue', 'en', 'nt', 'ti', 'ia', 'al', 'l ', ' w', 'wi', 'it', 'th', 'hi', 'in', 'n ', ' f', 'fr', 're', 'en', 'nc', 'ch', 'h ', ' w', 'wo', 'or', 'rk', 'ki', 'in', 'ng', 'g ', ' c', 'cl', 'la', 'as', 'ss', 's ', ' m', 'mo', 'ov', 've', 'em', 'me', 'en', 'nt', 'ts', 's ', ' a', 'an', 'nd', 'd ', ' h', 'hi', 'is', 's ', ' f', 'fo', 'ol', 'll', 'lo', 'ow', 'we', 'er', 'rs', 's ', ' w', 'we', 'er', 're', 'e ', ' a', 'ac', 'ct', 'ti', 'iv', 've', 'e ', ' i', 'in', 'n ', ' t', 'th', 'he', 'e ', ' r', 're', 'ev', 'vo', 'ol', 'lu', 'ut', 'ti', 'io', 'on', 'n ', ' o', 'of', 'f ', ' o', 'on', 'ne', 'e ', ' e', 'ei', 'ig', 'gh', 'ht', 't ', ' f', 'fo', 'ou', 'ur', 'r ', ' e', 'ei', 'ig', 'gh', 'ht', 't ', ' i', 'in', 'n ', ' f', 'fr', 'ra', 'an', 'nc', 'ce', 'e ', ' p', 'pr', 'ro', 'ou', 'ud', 'dh', 'ho', 'on', 'n ', ' s', 's ', ' p', 'ph', 'hi', 'il', 'lo', 'os', 'so', 'op', 'ph', 'hy', 'y ', ' o', 'of', 'f ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' i', 'is', 's ', ' c', 'co', 'om', 'mp', 'pl', 'le', 'ex', 'x ', ' i', 'it', 't ', ' w', 'wa', 'as', 's ', ' d', 'de', 'ev', 've', 'el', 'lo', 'op', 'pe', 'ed', 'd ', ' i', 'in', 'n ', ' a', 'a ', ' n', 'nu', 'um', 'mb', 'be', 'er', 'r ', ' o', 'of', 'f ', ' w', 'wo', 'or', 'rk', 'ks', 's ', ' o', 'ov', 've', 'er', 'r ', ' h', 'hi', 'is', 's ', ' l', 'li', 'if', 'fe', 'et', 'ti', 'im', 'me', 'e ', ' a', 'an', 'nd', 'd ', ' t', 'th', 'he', 'er', 're', 'e ', ' a', 'ar', 're', 'e ', ' d', 'di', 'if', 'ff', 'fe', 'er', 'ri', 'in', 'ng', 'g ', ' i', 'in', 'nt', 'te', 'er', 'rp', 'pr', 're', 'et', 'ta', 'at', 'ti', 'io', 'on', 'ns', 's ', ' o', 'of', 'f ', ' s', 'so', 'om', 'me', 'e ', ' o', 'of', 'f ', ' h', 'hi', 'is', 's ', ' i', 'id', 'de', 'ea', 'as', 's ', ' f', 'fo', 'or', 'r ', ' m', 'mo', 'or', 're', 'e ', ' d', 'de', 'et', 'ta', 'ai', 'il', 'le', 'ed', 'd ', ' d', 'di', 'is', 'sc', 'cu', 'us', 'ss', 'si', 'io', 'on', 'n ', ' s', 'se', 'ee', 'e ', ' h', 'he', 'er', 're', 'e ', ' m', 'ma', 'ax', 'x ', ' s', 'st', 'ti', 'ir', 'rn', 'ne', 'er', 'r ', ' s', 's ', ' e', 'eg', 'go', 'oi', 'is', 'sm', 'm ', ' i', 'in', 'n ', ' h', 'hi', 'is', 's ', ' t', 'th', 'he', 'e ', ' e', 'eg', 'go', 'o ', ' a', 'an', 'nd', 'd ', ' i', 'it', 'ts', 's ', ' o', 'ow', 'wn', 'n ', ' s', 'st', 'ti', 'ir', 'rn', 'ne', 'er', 'r ', ' a', 'ar', 'rg', 'gu', 'ue', 'ed', 'd ', ' t', 'th', 'ha', 'at', 't ', ' m', 'mo', 'os', 'st', 't ', ' c', 'co', 'om', 'mm', 'mo', 'on', 'nl', 'ly', 'y ', ' a', 'ac', 'cc', 'ce', 'ep', 'pt', 'te', 'ed', 'd ', ' s', 'so', 'oc', 'ci', 'ia', 'al', 'l ', ' i', 'in', 'ns', 'st', 'ti', 'it', 'tu', 'ut', 'ti', 'io', 'on', 'ns', 's ', ' i', 'in', 'nc', 'cl', 'lu', 'ud', 'di', 'in', 'ng', 'g ', ' t', 'th', 'he', 'e ', ' n', 'no', 'ot', 'ti', 'io', 'on', 'n ', ' o', 'of', 'f ', ' s', 'st', 'ta', 'at', 'te', 'e ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' a', 'as', 's ', ' a', 'a ', ' r', 'ri', 'ig', 'gh', 'ht', 't ', ' n', 'na', 'at', 'tu', 'ur', 'ra', 'al', 'l ', ' r', 'ri', 'ig', 'gh', 'ht', 'ts', 's ', ' i', 'in', 'n ', ' g', 'ge', 'en', 'ne', 'er', 'ra', 'al', 'l ', ' a', 'an', 'nd', 'd ', ' t', 'th', 'he', 'e ', ' v', 've', 'er', 'ry', 'y ', ' n', 'no', 'ot', 'ti', 'io', 'on', 'n ', ' o', 'of', 'f ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' w', 'we', 'er', 're', 'e ', ' m', 'me', 'er', 're', 'e ', ' i', 'il', 'll', 'lu', 'us', 'si', 'io', 'on', 'ns', 's ', ' o', 'or', 'r ', ' g', 'gh', 'ho', 'os', 'st', 'ts', 's ', ' i', 'in', 'n ', ' t', 'th', 'he', 'e ', ' m', 'mi', 'in', 'nd', 'd ', ' s', 'sa', 'ay', 'yi', 'in', 'ng', 'g ', ' o', 'of', 'f ', ' s', 'so', 'oc', 'ci', 'ie', 'et', 'ty', 'y ', ' t', 'th', 'ha', 'at', 't ', ' t', 'th', 'he', 'e ', ' i', 'in', 'nd', 'di', 'iv', 'vi', 'id', 'du', 'ua', 'al', 'ls', 's ', ' a', 'ar', 're', 'e ', ' i', 'it', 'ts', 's ', ' r', 're', 'ea', 'al', 'li', 'it', 'ty', 'y ', ' h', 'he', 'e ', ' a', 'ad', 'dv', 'vo', 'oc', 'ca', 'at', 'te', 'ed', 'd ', ' e', 'eg', 'go', 'oi', 'is', 'sm', 'm ', ' a', 'an', 'nd', 'd ', ' a', 'a ', ' f', 'fo', 'or', 'rm', 'm ', ' o', 'of', 'f ', ' a', 'am', 'mo', 'or', 'ra', 'al', 'li', 'is', 'sm', 'm ', ' i', 'in', 'n ', ' w', 'wh', 'hi', 'ic', 'ch', 'h ', ' i', 'in', 'nd', 'di', 'iv', 'vi', 'id', 'du', 'ua', 'al', 'ls', 's ', ' w', 'wo', 'ou', 'ul', 'ld', 'd ', ' u', 'un', 'ni', 'it', 'te', 'e ', ' i', 'in', 'n ', ' a', 'as', 'ss', 'so', 'oc', 'ci', 'ia', 'at', 'ti', 'io', 'on', 'ns', 's ', ' o', 'of', 'f ', ' e', 'eg', 'go', 'oi', 'is', 'st', 'ts', 's ', ' o', 'on', 'nl', 'ly', 'y ', ' w', 'wh', 'he', 'en', 'n ', ' i', 'it', 't ', ' w', 'wa', 'as', 's ', ' i', 'in', 'n ', ' t', 'th', 'he', 'ei', 'ir', 'r ', ' s', 'se', 'el', 'lf', 'f ', ' i', 'in', 'nt', 'te', 'er', 're', 'es', 'st', 't ', ' t', 'to', 'o ', ' d', 'do', 'o ', ' s', 'so', 'o ', ' f', 'fo', 'or', 'r ', ' h', 'hi', 'im', 'm ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' s', 'si', 'im', 'mp', 'pl', 'ly', 'y ', ' c', 'co', 'om', 'me', 'es', 's ', ' a', 'ab', 'bo', 'ou', 'ut', 't ', ' t', 'th', 'hr', 'ro', 'ou', 'ug', 'gh', 'h ', ' m', 'mi', 'ig', 'gh', 'ht', 't ', ' w', 'wh', 'ho', 'oe', 'ev', 've', 'er', 'r ', ' k', 'kn', 'no', 'ow', 'ws', 's ', ' h', 'ho', 'ow', 'w ', ' t', 'to', 'o ', ' t', 'ta', 'ak', 'ke', 'e ', ' t', 'to', 'o ', ' d', 'de', 'ef', 'fe', 'en', 'nd', 'd ', ' t', 'th', 'he', 'e ', ' t', 'th', 'hi', 'in', 'ng', 'g ', ' t', 'to', 'o ', ' h', 'hi', 'im', 'm ', ' b', 'be', 'el', 'lo', 'on', 'ng', 'gs', 's ', ' p', 'pr', 'ro', 'op', 'pe', 'er', 'rt', 'ty', 'y ', ' a', 'an', 'nd', 'd ', ' w', 'wh', 'ha', 'at', 't ', ' i', 'i ', ' h', 'ha', 'av', 've', 'e ', ' i', 'in', 'n ', ' m', 'my', 'y ', ' p', 'po', 'ow', 'we', 'er', 'r ', ' t', 'th', 'ha', 'at', 't ', ' i', 'is', 's ', ' m', 'my', 'y ', ' o', 'ow', 'wn', 'n ', ' s', 'so', 'o ', ' l', 'lo', 'on', 'ng', 'g ', ' a', 'as', 's ', ' i', 'i ', ' a', 'as', 'ss', 'se', 'er', 'rt', 't ', ' m', 'my', 'ys', 'se', 'el', 'lf', 'f ', ' a', 'as', 's ', ' h', 'ho', 'ol', 'ld', 'de', 'er', 'r ', ' i', 'i ', ' a', 'am', 'm ', ' t', 'th', 'he', 'e ', ' p', 'pr', 'ro', 'op', 'pr', 'ri', 'ie', 'et', 'to', 'or', 'r ', ' o', 'of', 'f ', ' t', 'th', 'he', 'e ', ' t', 'th', 'hi', 'in', 'ng', 'g ', ' s', 'st', 'ti', 'ir', 'rn', 'ne', 'er', 'r ', ' n', 'ne', 'ev', 've', 'er', 'r ', ' c', 'ca', 'al', 'll', 'le', 'ed', 'd ', ' h', 'hi', 'im', 'ms', 'se', 'el', 'lf', 'f ', ' a', 'an', 'n ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 't ', ' h', 'he', 'e ', ' a', 'ac', 'cc', 'ce', 'ep', 'pt', 'te', 'ed', 'd ', ' o', 'on', 'nl', 'ly', 'y ', ' t', 'th', 'he', 'e ', ' l', 'la', 'ab', 'be', 'el', 'l ', ' e', 'eg', 'go', 'oi', 'is', 'st', 't ', ' n', 'ne', 'ev', 've', 'er', 'rt', 'th', 'he', 'el', 'le', 'es', 'ss', 's ', ' h', 'hi', 'is', 's ', ' i', 'id', 'de', 'ea', 'as', 's ', ' w', 'we', 'er', 're', 'e ', ' i', 'in', 'nf', 'fl', 'lu', 'ue', 'en', 'nt', 'ti', 'ia', 'al', 'l ', ' o', 'on', 'n ', ' m', 'ma', 'an', 'ny', 'y ', ' i', 'in', 'nd', 'di', 'iv', 'vi', 'id', 'du', 'ua', 'al', 'li', 'is', 'st', 'ti', 'ic', 'ca', 'al', 'll', 'ly', 'y ', ' i', 'in', 'nc', 'cl', 'li', 'in', 'ne', 'ed', 'd ', ' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'st', 'ts', 's ', ' a', 'al', 'lt', 'th', 'ho', 'ou', 'ug', 'gh', 'h ', ' i', 'in', 'nt', 'te', 'er', 'rp', 'pr', 're', 'et', 'ta', 'at', 'ti', 'io', 'on', 'ns', 's ', ' o', 'of', 'f ', ' h', 'hi', 'is', 's ', ' t', 'th', 'ho', 'ou', 'ug', 'gh', 'ht', 't ', ' a', 'ar', 're', 'e ', ' d', 'di', 'iv', 've', 'er', 'rs', 'se']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "train_bigrams = bigrams(train_text)\n",
    "valid_bigrams = bigrams(valid_text)\n",
    "temp=list(train_bigrams)\n",
    "train_list=list()\n",
    "for tup in temp:\n",
    "    s=''.join(tup)\n",
    "    train_list.append(s)\n",
    "temp=list(valid_bigrams)\n",
    "valid_list=list()\n",
    "for tup in temp:\n",
    "    s=''.join(tup)\n",
    "    valid_list.append(s)\n",
    "print(valid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of words, it will preprocess and make batches to be fed to the skip-gram / CBOW model\n",
    "\n",
    "There are two functions - build_dataset ----> changes the word to decimal scalars(dictionary key-value pairs)\n",
    "\n",
    "(in the range specified by vocabulary_size (maybe 100s for letters and 1000s for words)\n",
    "\n",
    "generate_batch ------> give batch_size,num_skips and skip_window, it will give you back the ---> train batches and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0 # very important initialization, gets updated globally\n",
    "               #each time genarate batch is called.\n",
    "vocabulary_size = 500 \n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "def generate_batch(data,batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1) # continue till an unseen \n",
    "        #target is found\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 103163], ('e ', 3686082), (' t', 2449118), ('s ', 2222166), ('th', 1980414)]\n",
      "Sample data ['am', 'me', 'er', 'ri', 'ic']\n",
      "data: ['am', 'me', 'er', 'ri', 'ic', 'ca', 'an', 'n ']\n"
     ]
    }
   ],
   "source": [
    "train_data, count, dictionary, reverse_dictionary = build_dataset(train_list)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', [reverse_dictionary[di] for di in train_data[:5]])\n",
    "#del words  # Hint to reduce memory.\n",
    "print('data:', [reverse_dictionary[di] for di in train_data[:8]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'truediv:0' shape=(500, 128) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.(windowlen/2)\n",
    "num_skips = 2 # How many times to reuse a middle input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                               train_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  print(normalized_embeddings.get_shape)\n",
    "  #print(valid_embeddings.get_shape)\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.085847\n",
      "Nearest to ng:  y, su, nk, ld, vu, if, co, yb,\n",
      "Nearest to ti: sc, ov, rb, e , in, oo, dv, nf,\n",
      "Nearest to in: vi, nn, rf, is, z , mu, ot, ti,\n",
      "Nearest to wo: to, vo, xi, mr, gl, cm, vs, by,\n",
      "Nearest to ma: vu, nh, p , if, hc, dh, xy, oa,\n",
      "Nearest to g : yw, fs, ht, j , as, zy, eu, fl,\n",
      "Nearest to  d: ig, k , wr, rl, ar, if, nw, yl,\n",
      "Nearest to ur: bu, sm, ad, eq, bs, ck, r , pm,\n",
      "Nearest to fi: yl, ta, ra, ol, ty, gb, to, mn,\n",
      "Nearest to ou: fr, nk, eh, gu,  c, gr, zy, ml,\n",
      "Nearest to ce: l , wh, sn, jo, ez, ss, yo,  n,\n",
      "Nearest to  t: ao, uv, xa, ve, pn, xc, vo,  x,\n",
      "Nearest to ec: di, hl, tw, mb, ve, bl, ci, hs,\n",
      "Nearest to ve: rp,  t, ih, ec, nu, dd, an, c ,\n",
      "Nearest to ne: lb, kw, lh, sf, wl, c , qu, nj,\n",
      "Nearest to s : ta, ho, ih, ix, je, kh, ht, na,\n",
      "Average loss at step 2000: 1.870095\n",
      "Average loss at step 4000: 1.661086\n",
      "Average loss at step 6000: 1.636439\n",
      "Average loss at step 8000: 1.630484\n",
      "Average loss at step 10000: 1.619018\n",
      "Nearest to ng: n , rg, nt, nk, ug, vu, ig, og,\n",
      "Nearest to ti: t , te, to, th, tr, ts, ri, li,\n",
      "Nearest to in: on, an, en, is, io, un, ic, it,\n",
      "Nearest to wo: to, so, vo, wh, wi, cm, no, xi,\n",
      "Nearest to ma: ea, me, mo, vu,  a, oa, nh, va,\n",
      "Nearest to g : yw, n , j , d , zy, e , fs, f ,\n",
      "Nearest to  d:  i,  j,  g,  s,  l, ld,  o,  p,\n",
      "Nearest to ur: or, er, ar, ir, ul, ub, ut, nr,\n",
      "Nearest to fi: fa,  i, vi, yl, gi, gb, vy, fu,\n",
      "Nearest to ou: ol, or,  u, gu, cu, tu, of, o ,\n",
      "Nearest to ce: ch, te, ve, co, cr, re, he, cy,\n",
      "Nearest to  t:  a,  c,  i,  b,  h,  r,  s,  f,\n",
      "Nearest to ec:  c, oc, uc, hl, es, ic, mb, ea,\n",
      "Nearest to ve: vi, va, he, ce, vo, v , te, ie,\n",
      "Nearest to ne: he, na, no, nf, ni, ny, fe, ge,\n",
      "Nearest to s : st, h , d , n , sm, l , sc, x ,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(train_data,\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets modify the pre-processing step in LSTM to change from one-hot representation to word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'er', ' r', ' b', 'ri', 'is', 'te', ' w', 'ng', 'o ', ' i', 'ns', 'wo', 'ot', 'r ', 'to', 's ', ' d', 'di', ' c', 'th', 'op', 'ty', ' f', 's ', ' c', 'ed', 's ', 'at', 'te', 'to', 'd ', 'pe', ' g', 'oo', 'ar', 'us', 'ro', 'er', 'ri', 'ty', 'a ', 're', 'l ', 'nd', 'ag', 'ld', 'ia', 'ke', 'a ', 'sa', 'st', ' h', ' d', 'it', 'cs', 'co', 'nd', 'ce', 'te', 'ne', 'te', 're', 'dv']\n",
      "['me', 'ra', 're', 'be', 'ia', 'so', 'es', 'wa', 'g ', ' b', 'is', 'se', 'o ', 'ti', ' o', 'o ', ' w', 'di', 'it', 'ca', 'h ', 'pr', 'y ', 'fo', ' u', 'cl', 'd ', ' f', 't ', 'er', 'od', ' a', 'er', 'gu', 'op', 're', 'st', 'om', 'rt', 'ic', 'y ', ' s', 'en', ' l', 'd ', 'ga', 'd ', 'a ', 'en', ' t', 'ac', 'ti', 'ha', 'de', 'ti', 's ', 'on', 'd ', 'es', 'en', 'e ', 'e ', 'et', 'va']\n",
      "['er', 'al', 'em', 'ei', 'a ', 'on', 's ', 'as', ' i', 'be', 's ', 'e ', ' e', 'io', 'of', ' p', 'we', 'id', 'ti', 'au', ' s', 're', ' c', 'or', 'us', 'li', ' i', 'fi', ' s', 'rn', 'da', 'al', 'r ', 'ua', 'ps', 'e ', 't ', 'ma', 'ta', 'cu', ' t', 'si', 'nt', 'lo', ' v', 'an', ' h', ' s', 'n ', 'te', 'ch', 'it', 'as', 'ea', 'ic', ' t', 'nf', ' p', 's ', 'nd', ' n', ' w', 'ti', 'an']\n",
      "['ri', 'l ', 'mo', 'in', ' d', 'nm', ' w', 's ', 'in', 'el', ' m', ' f', 'ei', 'on', 'f ', 'pl', 'er', 'd ', 'io', 'us', 'st', 'es', 'cl', 'r ', 'su', 'ie', 'in', 'iv', 'su', 'na', 'ay', 'lb', ' i', 'at', 's ', ' s', ' p', 'an', 'ai', 'ul', 'to', 'im', 'ti', 'oa', 'vo', 'n ', 'he', 'si', ' b', 'el', 'hs', 'tc', 's ', 'at', 'ca', 'th', 'fr', 'pu', ' c', 'da', 'ni', 'wa', 'ic', 'nt']\n",
      "['ic', ' d', 'ov', 'ng', 'de', 'me', 'wi', ' i', 'n ', 'li', 'ma', 'fo', 'ig', 'n ', ' f', 'la', 're', ' f', 'on', 'sa', 'ta', 'se', 'la', ' c', 'ua', 'en', 'n ', 've', 'uc', 'at', 'y ', 'bu', 'in', 'tt', ' a', 'se', 'pr', 'n ', 'in', 'lt', 'o ', 'mp', 'ia', 'ad', 'ow', ' s', 'ea', 'ix', 'by', 'le', 's ', 'ch', ' f', 'th', 'al', 'he', 'ro', 'uz', 'co', 'an', 'in', 'as', 'c ', 'ta']\n",
      "['ca', 'di', 'vi', 'g ', 'e ', 'en', 'it', 'in', ' t', 'iz', 'ad', 'or', 'gh', ' s', 'fi', 'ay', 'e ', 'fo', 'n ', 'al', 'at', 'er', 'as', 'ce', 'al', 'nt', ' o', 'e ', 'ch', 'ti', ' h', 'um', 'nc', 'ta', 'ar', 'ev', 'ro', ' e', 'n ', 'tu', ' t', 'pl', 'al', 'd ', 'we', 'st', 'ad', 'x ', 'y ', 'et', ' h', 'he', 'fr', 'hs', 'l ', 'e ', 'on', 'zz', 'om', 'nt', 'ne', 's ', ' a', 'ag']\n",
      "['an', 'is', 'in', ' o', ' m', 'nt', 'th', 'n ', 'th', 'ze', 'de', 'rc', 'ht', 'sy', 'in', 'y ', ' o', 'or', ' p', 'l ', 'te', 'rv', 'ss', 'er', 'll', 't ', 'on', ' k', 'h ', 'iv', 'ho', 'm ', 'ch', 'ar', 're', 've', 'od', 'em', ' t', 'ur', 'th', 'le', 'l ', ' t', 'el', 'ta', 'UNK', ' f', ' d', 'th', 'ha', 'es', 'ro', 's ', ' r', ' s', 'nt', 'UNK', 'mm', 't ', 'e ', ' t', 'an', 'ge']\n",
      "['n ', 'st', 'ng', 'or', 'mo', 't ', 'h ', ' a', 'he', 'e ', 'e ', 'ce', 't ', 'ys', 'ne', ' a', 'oc', 'r ', 'pr', ' e', 'es', 'va', 'se', 'rt', 'ly', ' i', 'ne', 'km', ' a', 've', 'ow', ' m', 'h ', 'ri', 'e ', 'er', 'du', 'mp', 'tr', 're', 'he', 'e ', ' e', 'th', 'l ', 'at', 'qu', 'fo', 'dr', 'ho', 'av', 's ', 'om', ' f', 'ri', 'sc', 'ts', 'le', 'ma', ' l', ' o', 'th', 'nd', 'es']\n",
      "[' i', 'tr', 'g ', 'rb', 'ol', ' o', ' a', 'ab', 'e ', ' a', ' t', 'es', ' o', 'st', 'e ', 'a ', 'cc', ' s', 'ro', 'ex', 's ', 'at', 'es', 'ta', 'y ', 'in', 'e ', 'm ', 'as', 'e ', 'we', 'mo', ' h', 'i ', ' s', 'ra', 'uc', 'pi', 'ra', 'e ', 'e ', ' r', 'eq', 'hi', ' e', 'te', 'ua', 'ou', 'ri', 'on', 've', ' a', 'm ', 'fu', 'io', 'ci', 's ', 'es', 'an', 'le', 'on', 'he', 'd ', 's ']\n",
      "['in', 'ri', ' o', 'bi', 'li', 'of', 'as', 'be', ' p', 'a ', 'to', 's ', 'on', 'te', ' s', ' g', 'cu', 'so', 'oj', 'xp', ' a', 'ti', 's ', 'ai', ' a', 'nt', ' n', ' l', 's ', ' p', 'ev', 'om', 'ho', ' i', 'se', 'al', 'ce', 'ir', 'ad', ' b', ' n', 're', 'qu', 'is', 'em', 'ed', 'ar', 'ur', 'iv', 'n ', 'e ', 'ac', ' e', 'UNK', 'ot', 'ie', ' h', 's ', 'nd', 'ea', 'ne', 'e ', ' a', ' o']\n",
      "['nd', 'ic', 'on', 'it', 'in', 'f ', 'ss', 'ey', 'pa', ' h', 'o ', ' t', 'ne', 'em', 'st', 'ga', 'up', 'om', 'je', 'pl', 'an', 'io', ' c', 'in', 'a ', 'te', 'ni', 'lo', ' i', 'pr', 've', 'me', 'or', 'in', 'en', 'l ', 'e ', 're', 'di', 'be', 'na', 'el', 'ua', 's ', 'mp', 'd ', 'rt', 'r ', 've', ' l', ' c', 'cr', 'ea', 'ji', 'ts', 'en', 'hi', ' a', 'd ', 'ar', 'e ', ' f', 'ac', 'of']\n",
      "['ammeerriiccaann  iinnd', 'erraall  ddiissttrriic', ' rreemmoovviinngg  oon', ' bbeeiinngg  oorrbbiit', 'riiaa  ddee  mmoolliin', 'issoonnmmeenntt  ooff ', 'teess  wwiitthh  aasss', ' wwaass  iinn  aabbeey', 'ngg  iinn  tthhee  ppa', 'o  bbeelliizzee  aa  h', ' iiss  mmaaddee  ttoo ', 'nssee  ffoorrcceess  t', 'woo  eeiigghhtt  oonne', 'ottiioonn  ssyysstteem', 'r  ooff  ffiinnee  sst', 'too  ppllaayy  aa  gga', 's  wweerree  ooccccuup', ' ddiidd  ffoorr  ssoom', 'diittiioonn  pprroojje', ' ccaauussaall  eexxppl', 'thh  ssttaatteess  aan', 'opprreesseerrvvaattiio', 'tyy  ccllaasssseess  c', ' ffoorr  cceerrttaaiin', 's  uussuuaallllyy  aa ', ' cclliieenntt  iinntte', 'edd  iinn  oonnee  nni', 's  ffiivvee  kkmm  llo', 'att  ssuucchh  aass  i', 'teerrnnaattiivvee  ppr', 'tooddaayy  hhoowweevve', 'd  aallbbuumm  mmoomme', 'peerr  iinncchh  hhoor', ' gguuaattttaarrii  iin', 'oooppss  aarree  sseen', 'arree  sseevveerraall ', 'usstt  pprroodduuccee ', 'roommaann  eemmppiirre', 'errttaaiinn  ttrraaddi', 'riiccuullttuurree  bbe', 'tyy  ttoo  tthhee  nna', 'a  ssiimmppllee  rreel', 'reennttiiaall  eeqquua', 'l  llooaadd  tthhiiss ', 'ndd  vvoowweell  eemmp', 'aggaann  ssttaatteedd ', 'ldd  hheeaadUNKquuaarrt', 'iaa  ssiixx  ffoouurr ', 'keenn  bbyy  ddrriivve', 'a  tteelleetthhoonn  l', 'saacchhss  hhaavvee  c', 'sttiittcchheess  aaccr', ' hhaass  ffrroomm  eea', ' ddeeaatthhss  ffuUNKji', 'ittiiccaall  rriiootts', 'css  tthhee  sscciieen', 'coonnffrroonnttss  hhi', 'ndd  ppuuzzzUNKleess  a', 'ceess  ccoommmmaanndd ', 'teennddaanntt  lleeaar', 'nee  nniinnee  oonnee ', 'tee  wwaass  tthhee  f', 'reettiicc  aanndd  aac', 'dvvaannttaaggeess  oof']\n",
      "['nd', 'ic', 'on', 'it', 'in', 'f ', 'ss', 'ey', 'pa', ' h', 'o ', ' t', 'ne', 'em', 'st', 'ga', 'up', 'om', 'je', 'pl', 'an', 'io', ' c', 'in', 'a ', 'te', 'ni', 'lo', ' i', 'pr', 've', 'me', 'or', 'in', 'en', 'l ', 'e ', 're', 'di', 'be', 'na', 'el', 'ua', 's ', 'mp', 'd ', 'rt', 'r ', 've', ' l', ' c', 'cr', 'ea', 'ji', 'ts', 'en', 'hi', ' a', 'd ', 'ar', 'e ', ' f', 'ac', 'of']\n",
      "['di', 'ct', 'ne', 'te', 'na', ' h', 'sy', 'ya', 'as', 'ha', ' p', 'th', 'e ', 'm ', 'to', 'am', 'pi', 'me', 'ec', 'la', 'nd', 'on', 'ca', 'n ', ' d', 'el', 'in', 'on', 'in', 'ro', 'er', 'en', 'ri', 'n ', 'nt', ' t', ' t', 'e ', 'it', 'ei', 'at', 'li', 'at', ' c', 'ph', ' t', 'te', ' m', 'er', 'le', 'cl', 'ro', 'ar', 'UNK', 's ', 'nc', 'is', 'a ', ' c', 'rn', ' b', 'fi', 'cc', 'f ']\n",
      "['iv', 't ', 'e ', 'ed', 'a ', 'hu', 'yr', 'an', 'st', 'an', 'pl', 'he', ' n', ' n', 'on', 'me', 'ie', 'e ', 'ct', 'an', 'd ', 'n ', 'an', ' e', 'de', 'll', 'ne', 'ng', 'n ', 'ot', 'r ', 'nt', 'iz', ' f', 't ', 'th', 'th', ' e', 'ti', 'in', 'ti', 'ig', 'ti', 'ca', 'ha', 'th', 'er', 'mo', 'r ', 'ew', 'la', 'os', 'rl', 'wa', ' f', 'ce', 's ', ' c', 'co', 'ni', 'by', 'ir', 'co', ' m']\n",
      "['vi', ' i', ' o', 'd ', ' t', 'un', 'ri', 'nc', 't ', 'nd', 'la', 'e ', 'ni', 'no', 'ne', 'e ', 'ed', ' o', 't ', 'na', ' i', ' b', 'n ', 'ev', 'ee', 'li', 'e ', 'g ', ' t', 'to', ' i', 't ', 'zo', 'fr', ' t', 'he', 'he', 'ec', 'io', 'ng', 'io', 'gi', 'io', 'an', 'as', 'ha', 'rs', 'od', ' r', 'wi', 'ai', 'ss', 'ly', 'ar', 'fo', 'e ', ' r', 'co', 'on', 'in', 'y ', 'rs', 'om', 'mp']\n",
      "['id', 'is', 'of', ' b', 'th', 'nd', 'ia', 'ce', ' t', 'db', 'ay', ' u', 'in', 'ot', 'e ', ' b', 'd ', 'of', ' i', 'at', 'is', 'bl', ' b', 've', 'ep', 'ig', ' n', ' a', 'th', 'oc', 'it', ' o', 'on', 're', 'to', 'eo', 'e ', 'co', 'on', 'g ', 'on', 'io', 'on', 'n ', 'si', 'at', 's ', 'de', 'ru', 'is', 'im', 's ', 'y ', 'ra', 'ol', ' w', 're', 'ol', 'nt', 'ng', ' a', 'st', 'mm', 'pl']\n",
      "['du', 's ', 'f ', 'by', 'he', 'dr', 'an', 'e ', 'th', 'bo', 'y ', 'un', 'ne', 't ', ' b', 'by', ' a', 'f ', 'in', 'ti', 's ', 'lo', 'be', 'en', 'p ', 'ge', 'ni', 'an', 'he', 'co', 't ', 'of', 'nt', 'en', 'o ', 'or', ' s', 'on', 'na', ' c', 'n ', 'ou', 'n ', ' i', 'is', 't ', ' o', 'e ', 'ud', 's ', 'me', ' e', ' o', 'a ', 'll', 'wh', 'ef', 'll', 'tr', 'g ', 'a ', 't ', 'mo', 'ls']\n",
      "['ua', ' c', ' t', 'y ', 'e ', 're', 'n ', ' f', 'hr', 'oo', ' c', 'ni', 'e ', ' b', 'bu', 'y ', 'at', ' h', 'nt', 'io', ' c', 'oo', 'e ', 'nt', ' b', 'en', 'in', 'nd', 'e ', 'ol', ' h', 'f ', 'ta', 'nc', ' s', 'ri', 'sa', 'no', 'al', 'ci', ' b', 'us', ' i', 'im', 's ', ' h', 'on', ' i', 'do', ' h', 'ed', 'ea', 'on', ' f', 'lo', 'hi', 'fl', 'le', 'ro', ' t', ' t', ' t', 'od', 's ']\n",
      "['al', 'ci', 'te', ' a', ' f', 'ed', ' a', 'fo', 're', 'ok', 'ch', 'it', ' o', 'ba', 'ui', ' a', 't ', 'hi', 'tr', 'on', 'co', 'od', ' c', 'ts', 'bl', 'nc', 'ne', 'd ', ' n', 'ls', 'ha', ' t', 'al', 'ch', 'so', 'ie', 'am', 'om', 'l ', 'iv', 'by', 's ', 'is', 'mp', ' p', 'hi', 'ne', 'it', 'ol', 'ha', 'd ', 'ac', 'n ', 'fa', 'ow', 'ic', 'le', 'ec', 'ol', 'th', 'te', 'to', 'da', ' s']\n",
      "['li', 'iu', 'en', 'a ', 'fi', 'ds', 'am', 'or', 'ee', 'k ', 'ho', 'te', 'on', 'as', 'il', 'a ', ' v', 'is', 'ro', 'n ', 'on', 'd ', 'ch', 's ', 'lu', 'ce', 'e ', ' c', 'ne', 's ', 'as', 'tr', 'l ', 'h ', 'ou', 'es', 'me', 'mi', ' p', 'vi', 'y ', ' s', 's ', 'pr', 'pe', 'is', 'e ', 't ', 'lf', 'as', ' t', 'ch', ' b', 'am', 'wi', 'ch', 'ec', 'ct', 'ls', 'he', 'ea', 'o ', 'at', 'su']\n",
      "['is', 'ud', 'n ', ' p', 'ir', 's ', 'mb', 'r ', 'e ', ' o', 'or', 'ed', 'ne', 'se', 'ld', ' s', 'va', 's ', 'od', ' w', 'ns', ' s', 'ha', ' t', 'ue', 'e ', ' e', 'co', 'et', ' a', 's ', 'ru', ' f', ' o', 'ut', 's ', 'e ', 'ic', 'pr', 'il', ' v', 'sy', ' x', 'ro', 'er', 's ', ' s', ' p', 'f ', 's ', 'th', 'h ', 'be', 'mi', 'in', 'h ', 'ct', 'ti', 's ', 'e ', 'am', ' e', 'ti', 'uc']\n",
      "['st', 'da', ' l', 'po', 'rs', ' o', 'ba', ' o', ' z', 'on', 'rd', 'd ', 'e ', 'ed', 'di', 'se', 'ar', ' c', 'du', 'wo', 'si', 'sp', 'ar', 'to', 'e ', ' t', 'ei', 'on', 'th', 'an', ' c', 'ut', 'fo', 'on', 'th', ' t', ' r', 'c ', 'ra', 'li', 'vi', 'ys', 'x ', 'ov', 'rs', ' p', 'si', 'pe', ' c', ' o', 'ha', ' o', 'ee', 'il', 'ng', ' s', 'ti', 'io', ' o', ' t', 'm ', 'el', 'in', 'ch']\n",
      "['nddiivviidduuaalliisst', 'icctt  iiss  cciiuudda', 'onnee  ooff  tteenn  l', 'itteedd  bbyy  aa  ppo', 'innaa  tthhee  ffiirrs', 'f  hhuunnddrreeddss  o', 'sssyyrriiaann  aammbba', 'eyyaannccee  ffoorr  o', 'paasstt  tthhrreeee  z', ' hhaannddbbooookk  oon', 'o  ppllaayy  cchhoorrd', ' tthhee  uunniitteedd ', 'nee  nniinnee  oonnee ', 'emm  nnoott  bbaasseed', 'sttoonnee  bbuuiillddi', 'gaammee  bbyy  aa  sse', 'uppiieedd  aatt  vvaar', 'ommee  ooff  hhiiss  c', 'jeecctt  iinnttrrooddu', 'pllaannaattiioonn  wwo', 'anndd  iiss  ccoonnssi', 'ioonn  bblloooodd  ssp', ' ccaann  bbee  cchhaar', 'inn  eevveennttss  tto', 'a  ddeeeepp  bblluuee ', 'teelllliiggeennccee  t', 'niinnee  nniinnee  eei', 'loonngg  aanndd  ccoon', ' iinn  tthhee  nneetth', 'prroottooccoollss  aan', 'veerr  iitt  hhaass  c', 'meenntt  ooff  ttrruut', 'orriizzoonnttaall  ffo', 'inn  ffrreenncchh  oon', 'enntt  ttoo  ssoouutth', 'l  tthheeoorriieess  t', 'e  tthhee  ssaammee  r', 'ree  eeccoonnoommiicc ', 'diittiioonnaall  pprra', 'beeiinngg  cciivviilli', 'naattiioonn  bbyy  vvi', 'elliiggiioouuss  ssyys', 'uaattiioonn  iiss  xx ', 's  ccaann  iimmpprroov', 'mpphhaassiiss  ppeerrs', 'd  tthhaatt  hhiiss  p', 'rtteerrss  oonnee  ssi', 'r  mmooddee  iitt  ppe', 'veerr  rruuddoollff  c', ' lleewwiiss  hhaass  o', ' ccllaaiimmeedd  tthha', 'crroossss  eeaacchh  o', 'eaarrllyy  oonn  bbeee', 'jiUNKwaarraa  ffaammiil', 'tss  ffoolllloowwiinng', 'ennccee  wwhhiicchh  s', 'hiiss  rreefflleecctti', ' aa  ccoolllleeccttiio', 'd  ccoonnttrroollss  o', 'arrnniinngg  tthhee  t', 'e  bbyy  aa  tteeaamm ', ' ffiirrsstt  ttoo  eel', 'accccoommmmooddaattiin', 'off  mmppllss  ssuucch']\n"
     ]
    }
   ],
   "source": [
    "#train=[reverse_dictionary[di] for di in train_data]\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "        \n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batchtemp = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batchtemp[b] = self._text[self._cursor[b]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    batch=wrdVec(batchtemp)\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def wrdVec(ids):\n",
    "    graph1=tf.Graph()\n",
    "    with graph1.as_default():\n",
    "        v = tf.constant(ids, dtype=tf.int32)\n",
    "        e=tf.constant(final_embeddings,dtype=tf.float32)\n",
    "        c=tf.nn.embedding_lookup(e,v)\n",
    "    with tf.Session(graph=graph1) as sess:\n",
    "        return sess.run(c)\n",
    "    \n",
    "def VecWrdBatch(vectors):\n",
    "    b=[]\n",
    "    for vector in vectors:\n",
    "        #print(vector.shape)\n",
    "        v=np.array(vector)\n",
    "        res=np.dot(final_embeddings,v)\n",
    "            #print(v.shape)\n",
    "        b.append(reverse_dictionary[np.argmax(res,0)])\n",
    "    return(b)\n",
    "def VecWrd(vector):\n",
    "    v=np.array(vector)\n",
    "    res=np.dot(final_embeddings,v)\n",
    "    b=np.argmax(res,0)\n",
    "    return(reverse_dictionary[b])\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    \n",
    "    print(VecWrdBatch(b))\n",
    "    #l=[reverse_dictionary[x] for x in VecWrdBatch(b)]\n",
    "    #print(list(zip(s,VecWrdBatch(b))))\n",
    "    #for x in zip(s,VecWrdBatch(b)):\n",
    "     #   print(x)\n",
    "    s = [''.join(x) for x in zip(s,VecWrdBatch(b))]\n",
    "    #s = [''.join(reverse_dictionary[x]) for x in VecWrdBatch(b)]\n",
    "    #s = [x for x in VecWrd(bi)]\n",
    "    #print(s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_data, batch_size, num_unrollings)\n",
    "#valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "#print(batches2string(train_batches._last_batch))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#train_batches._last_batch\n",
    "#train_batches._last_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-40-dee96784e2de>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-dee96784e2de>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    return i\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:l\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIP :  Actual output y(t)  and lstm cell output h(t) are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  print(train_labels)\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  k=1\n",
    "  for i in train_inputs:\n",
    "    if k==1:\n",
    "        print(i)\n",
    "    k=k+1\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "    print(len(outputs))\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    print(tf.concat(0, outputs))\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0,train_labels), logits=logits))\n",
    "    print(tf.concat(0, train_labels))\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.(without\n",
    "# creating another graph)\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    " ld     [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
