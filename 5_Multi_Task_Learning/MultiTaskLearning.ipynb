{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set 1 (57274, 957) (57274,)\n",
      "Training set 2 (117280, 957) (117280,)\n",
      "Validation set (6363, 957) (6363,)\n",
      "Test set (2449, 957) (2449,)\n",
      "Minibatch loss task1 at step 0: 0.164956\n",
      "Minibatch loss task2 at step 0: 0.272763\n",
      "Minibatch accuracy task 1: 93.3%\n",
      "Minibatch accuracy task 2: 92.4%\n",
      "Validation accuracy: 92.4%\n",
      "Test accuracy: 81.6%\n",
      "Minibatch loss task1 at step 300: 0.164587\n",
      "Minibatch loss task2 at step 300: 0.242000\n",
      "Minibatch accuracy task 1: 93.7%\n",
      "Minibatch accuracy task 2: 93.3%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 81.4%\n",
      "Minibatch loss task1 at step 600: 0.151284\n",
      "Minibatch loss task2 at step 600: 0.235301\n",
      "Minibatch accuracy task 1: 93.3%\n",
      "Minibatch accuracy task 2: 93.2%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 81.9%\n",
      "Minibatch loss task1 at step 900: 0.147333\n",
      "Minibatch loss task2 at step 900: 0.233587\n",
      "Minibatch accuracy task 1: 93.6%\n",
      "Minibatch accuracy task 2: 93.4%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 81.9%\n",
      "Minibatch loss task1 at step 1200: 0.146597\n",
      "Minibatch loss task2 at step 1200: 0.217427\n",
      "Minibatch accuracy task 1: 94.5%\n",
      "Minibatch accuracy task 2: 93.7%\n",
      "Validation accuracy: 92.5%\n",
      "Test accuracy: 82.4%\n",
      "Minibatch loss task1 at step 1500: 0.156367\n",
      "Minibatch loss task2 at step 1500: 0.196302\n",
      "Minibatch accuracy task 1: 93.4%\n",
      "Minibatch accuracy task 2: 94.0%\n",
      "Validation accuracy: 92.8%\n",
      "Test accuracy: 82.3%\n",
      "Minibatch loss task1 at step 1800: 0.145795\n",
      "Minibatch loss task2 at step 1800: 0.201514\n",
      "Minibatch accuracy task 1: 94.2%\n",
      "Minibatch accuracy task 2: 93.9%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 83.5%\n",
      "Minibatch loss task1 at step 2100: 0.160713\n",
      "Minibatch loss task2 at step 2100: 0.207808\n",
      "Minibatch accuracy task 1: 93.3%\n",
      "Minibatch accuracy task 2: 93.9%\n",
      "Validation accuracy: 92.8%\n",
      "Test accuracy: 83.0%\n",
      "Minibatch loss task1 at step 2400: 0.135847\n",
      "Minibatch loss task2 at step 2400: 0.217757\n",
      "Minibatch accuracy task 1: 94.6%\n",
      "Minibatch accuracy task 2: 94.0%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 82.6%\n",
      "Minibatch loss task1 at step 2700: 0.142000\n",
      "Minibatch loss task2 at step 2700: 0.186174\n",
      "Minibatch accuracy task 1: 94.1%\n",
      "Minibatch accuracy task 2: 94.4%\n",
      "Validation accuracy: 92.8%\n",
      "Test accuracy: 83.7%\n",
      "Minibatch loss task1 at step 3000: 0.144737\n",
      "Minibatch loss task2 at step 3000: 0.188822\n",
      "Minibatch accuracy task 1: 93.6%\n",
      "Minibatch accuracy task 2: 94.4%\n",
      "Validation accuracy: 92.9%\n",
      "Test accuracy: 83.9%\n",
      "Minibatch loss task1 at step 3300: 0.146139\n",
      "Minibatch loss task2 at step 3300: 0.188989\n",
      "Minibatch accuracy task 1: 94.2%\n",
      "Minibatch accuracy task 2: 94.7%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 83.5%\n",
      "Minibatch loss task1 at step 3600: 0.150113\n",
      "Minibatch loss task2 at step 3600: 0.189798\n",
      "Minibatch accuracy task 1: 94.2%\n",
      "Minibatch accuracy task 2: 94.7%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 84.0%\n",
      "Minibatch loss task1 at step 3900: 0.143600\n",
      "Minibatch loss task2 at step 3900: 0.180225\n",
      "Minibatch accuracy task 1: 94.1%\n",
      "Minibatch accuracy task 2: 94.7%\n",
      "Validation accuracy: 92.7%\n",
      "Test accuracy: 84.3%\n",
      "Minibatch loss task1 at step 4200: 0.144444\n",
      "Minibatch loss task2 at step 4200: 0.167050\n",
      "Minibatch accuracy task 1: 93.9%\n",
      "Minibatch accuracy task 2: 95.0%\n",
      "Validation accuracy: 92.8%\n",
      "Test accuracy: 84.3%\n",
      "Minibatch loss task1 at step 4500: 0.140310\n",
      "Minibatch loss task2 at step 4500: 0.168332\n",
      "Minibatch accuracy task 1: 94.1%\n",
      "Minibatch accuracy task 2: 95.0%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 84.2%\n",
      "Minibatch loss task1 at step 4800: 0.146070\n",
      "Minibatch loss task2 at step 4800: 0.179693\n",
      "Minibatch accuracy task 1: 93.9%\n",
      "Minibatch accuracy task 2: 94.9%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 84.5%\n",
      "Minibatch loss task1 at step 5100: 0.136379\n",
      "Minibatch loss task2 at step 5100: 0.164486\n",
      "Minibatch accuracy task 1: 94.5%\n",
      "Minibatch accuracy task 2: 95.1%\n",
      "Validation accuracy: 93.0%\n",
      "Test accuracy: 84.2%\n",
      "Minibatch loss task1 at step 5400: 0.141970\n",
      "Minibatch loss task2 at step 5400: 0.165959\n",
      "Minibatch accuracy task 1: 93.9%\n",
      "Minibatch accuracy task 2: 94.8%\n",
      "Validation accuracy: 93.3%\n",
      "Test accuracy: 84.5%\n",
      "Minibatch loss task1 at step 5700: 0.146577\n",
      "Minibatch loss task2 at step 5700: 0.151541\n",
      "Minibatch accuracy task 1: 93.8%\n",
      "Minibatch accuracy task 2: 95.3%\n",
      "Validation accuracy: 93.3%\n",
      "Test accuracy: 84.2%\n",
      "Minibatch loss task1 at step 6000: 0.141126\n",
      "Minibatch loss task2 at step 6000: 0.158078\n",
      "Minibatch accuracy task 1: 93.9%\n",
      "Minibatch accuracy task 2: 95.3%\n",
      "Validation accuracy: 93.4%\n",
      "Test accuracy: 84.6%\n",
      "Minibatch loss task1 at step 6300: 0.142731\n",
      "Minibatch loss task2 at step 6300: 0.166739\n",
      "Minibatch accuracy task 1: 94.2%\n",
      "Minibatch accuracy task 2: 95.0%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 84.6%\n",
      "Minibatch loss task1 at step 6600: 0.130556\n",
      "Minibatch loss task2 at step 6600: 0.155176\n",
      "Minibatch accuracy task 1: 94.5%\n",
      "Minibatch accuracy task 2: 95.1%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 85.0%\n",
      "Minibatch loss task1 at step 6900: 0.146376\n",
      "Minibatch loss task2 at step 6900: 0.141494\n",
      "Minibatch accuracy task 1: 94.3%\n",
      "Minibatch accuracy task 2: 95.5%\n",
      "Validation accuracy: 93.4%\n",
      "Test accuracy: 84.6%\n",
      "Minibatch loss task1 at step 7200: 0.134305\n",
      "Minibatch loss task2 at step 7200: 0.156383\n",
      "Minibatch accuracy task 1: 94.5%\n",
      "Minibatch accuracy task 2: 95.4%\n",
      "Validation accuracy: 93.0%\n",
      "Test accuracy: 84.9%\n",
      "Minibatch loss task1 at step 7500: 0.135866\n",
      "Minibatch loss task2 at step 7500: 0.148936\n",
      "Minibatch accuracy task 1: 94.3%\n",
      "Minibatch accuracy task 2: 95.5%\n",
      "Validation accuracy: 93.1%\n",
      "Test accuracy: 84.4%\n",
      "Minibatch loss task1 at step 7800: 0.134700\n",
      "Minibatch loss task2 at step 7800: 0.135479\n",
      "Minibatch accuracy task 1: 94.5%\n",
      "Minibatch accuracy task 2: 95.8%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 84.7%\n",
      "Minibatch loss task1 at step 8100: 0.132965\n",
      "Minibatch loss task2 at step 8100: 0.156346\n",
      "Minibatch accuracy task 1: 94.5%\n",
      "Minibatch accuracy task 2: 95.2%\n",
      "Validation accuracy: 93.1%\n",
      "Test accuracy: 84.7%\n",
      "Minibatch loss task1 at step 8400: 0.136546\n",
      "Minibatch loss task2 at step 8400: 0.138412\n",
      "Minibatch accuracy task 1: 94.5%\n",
      "Minibatch accuracy task 2: 95.7%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 84.9%\n",
      "Minibatch loss task1 at step 8700: 0.131178\n",
      "Minibatch loss task2 at step 8700: 0.149279\n",
      "Minibatch accuracy task 1: 94.8%\n",
      "Minibatch accuracy task 2: 95.8%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 84.6%\n",
      "Minibatch loss task1 at step 9000: 0.144839\n",
      "Minibatch loss task2 at step 9000: 0.150178\n",
      "Minibatch accuracy task 1: 94.0%\n",
      "Minibatch accuracy task 2: 95.8%\n",
      "Validation accuracy: 93.3%\n",
      "Test accuracy: 84.9%\n",
      "Minibatch loss task1 at step 9300: 0.139194\n",
      "Minibatch loss task2 at step 9300: 0.130267\n",
      "Minibatch accuracy task 1: 94.4%\n",
      "Minibatch accuracy task 2: 96.0%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 84.9%\n",
      "Minibatch loss task1 at step 9600: 0.137384\n",
      "Minibatch loss task2 at step 9600: 0.155002\n",
      "Minibatch accuracy task 1: 94.6%\n",
      "Minibatch accuracy task 2: 95.7%\n",
      "Validation accuracy: 93.1%\n",
      "Test accuracy: 84.9%\n",
      "Minibatch loss task1 at step 9900: 0.132817\n",
      "Minibatch loss task2 at step 9900: 0.138918\n",
      "Minibatch accuracy task 1: 94.3%\n",
      "Minibatch accuracy task 2: 95.9%\n",
      "Validation accuracy: 93.1%\n",
      "Test accuracy: 85.6%\n",
      "[[ -1.36321676e+00   1.95362520e+00  -1.83318570e-01 ...,  -4.29907113e-01\n",
      "    1.54483601e-01  -2.22704664e-01]\n",
      " [ -9.47309673e-01   3.53286952e-01   2.16275311e+00 ...,   6.70921624e-01\n",
      "   -3.57106924e+00   1.48414469e+00]\n",
      " [  2.44783545e+00   6.15337610e-01  -3.75828934e+00 ...,   2.97271460e-01\n",
      "   -8.42509195e-02   1.10609233e+00]\n",
      " ..., \n",
      " [ -2.12855358e-02  -1.06249750e+00  -1.16495156e+00 ...,   6.02592945e-01\n",
      "    2.64543843e+00  -1.57764864e+00]\n",
      " [  1.20393217e+00   1.21086523e-01  -4.76744980e-01 ...,  -2.33680755e-01\n",
      "   -9.33448609e-04   1.40579715e-01]\n",
      " [ -4.00259852e-01  -1.82647836e+00   6.90914690e-01 ...,   5.38814366e-01\n",
      "    8.71739149e-01   1.83775449e+00]]\n"
     ]
    }
   ],
   "source": [
    "# %load '/home/dnnlab/vaisakh/anaconda3/Code/LOG/dnnDynamicTF.py'\n",
    "\"\"\"\n",
    "Created on Sun Dec 1 2:23:09 2016\n",
    "\n",
    "@author: Vaisakh \n",
    "email: vaisakhs.shaj@gmail.com\n",
    "\"\"\"\n",
    "\n",
    "############## Import Libraries Here###################\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "\n",
    "class FLAGS:\n",
    "    def __init__(self):\n",
    "        self.randomInit = True\n",
    "        self.b = 0\n",
    "        \n",
    "############# Set Parameters here #####################\n",
    "FLAG =FLAGS()\n",
    "FLAG.randomInit = False\n",
    "checkPointPath='/home/dnnlab/vaisakh/familyClassification/Model/45layerMTL.ckpt'\n",
    "checkPointPath2='/home/dnnlab/vaisakh/familyClassification/Model/45layerMTL.ckpt'\n",
    "checkPointFolder='/home/dnnlab/vaisakh/familyClassification/Model'\n",
    "#input_size = 1270\n",
    "input_size=957\n",
    "num_labels_1 = 2\n",
    "num_labels_2 = 21\n",
    "\n",
    "batch_size_1 =5000\n",
    "batch_size_2 =10000\n",
    "#H=[ 1525, 1018] #hidden layer size\n",
    "H1=[ 1100, 760] #hidden layer size\n",
    "H2= [ 1100,760,480 ]       \n",
    "#print(num_labels)\n",
    "lam=0\n",
    "num_steps = 10001\n",
    "learn_rate=0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ Load Data and preprocess\n",
    "\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/xMillionTrainL1-2001.pickle\",\"rb\")\n",
    "train_dataset1=pickle.load(pickle_in)\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/yMillionTrainL1-2001.pickle\",\"rb\")\n",
    "train_labels1=pickle.load(pickle_in)\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/xMillionTrainL1-21.pickle\",\"rb\")\n",
    "train_dataset2=pickle.load(pickle_in)\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/yMillionTrainL1-21.pickle\",\"rb\")\n",
    "train_labels2=pickle.load(pickle_in)\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/xMillionCrossL1-2001.pickle\",\"rb\")\n",
    "valid_dataset=pickle.load(pickle_in)\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/yMillionCrossL1-2001.pickle\",\"rb\")\n",
    "valid_labels=pickle.load(pickle_in)\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/xMillionCrossL1-21.pickle\",\"rb\")\n",
    "test_dataset=pickle.load(pickle_in)\n",
    "pickle_in=open(\"/home/dnnlab/vaisakh/familyClassification/Data/yMillionCrossL1-21.pickle\",\"rb\")\n",
    "test_labels=pickle.load(pickle_in)\n",
    "\n",
    "print('Training set 1', train_dataset1.shape, train_labels1.shape)\n",
    "print('Training set 2', train_dataset2.shape, train_labels2.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "##=================Preprocessing\n",
    "\n",
    "\n",
    "train_labels1=(np.arange(num_labels_1)==train_labels1[:,None].astype(np.float32))\n",
    "train_labels2=(np.arange(num_labels_2)==train_labels2[:,None].astype(np.float32))\n",
    "valid_labels=(np.arange(num_labels_1)==valid_labels[:,None].astype(np.float32))\n",
    "test_labels=(np.arange(num_labels_2)==test_labels[:,None].astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "############ Training ################################################################l\n",
    "def predictTask1(X,w1,w2,w3,b1,b2,b3):\n",
    "     layer2= tf.matmul(X, w1) + b1\n",
    "     layer2out = tf.nn.relu(layer2)\n",
    "     layer3 = tf.matmul(layer2out, w2) + b2\n",
    "     layer3out = tf.nn.relu(layer3)\n",
    "     layer4 = tf.matmul(layer3out, w3) + b3\n",
    "     layer4out = tf.nn.softmax(layer4)\n",
    "     return layer4out\n",
    "    \n",
    "def predictTask2(X,w1,w2,w3,w4,b1,b2,b3,b4):\n",
    "     layer2= tf.matmul(X, w1) + b1\n",
    "     layer2out = tf.nn.relu(layer2)\n",
    "     layer3 = tf.matmul(layer2out, w2) + b2\n",
    "     layer3out = tf.nn.relu(layer3)\n",
    "     layer4 = tf.matmul(layer3out, w3) + b3\n",
    "     layer4out = tf.nn.relu(layer4)\n",
    "     layer5 = tf.matmul(layer4out, w4) + b4\n",
    "     layer5out = tf.nn.softmax(layer5)\n",
    "     return layer5out\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def accuracyTest(predictions, labels):\n",
    "  acti=np.max(predictions, 1)\n",
    "  yPr=np.argmax(predictions, 1)\n",
    "  yAc=np.argmax(labels, 1)\n",
    "  print(confusion_matrix(yAc,yPr))\n",
    "  print(accuracy_score(yAc,yPr))\n",
    "  return 1\n",
    "###=====================Defining Graphs========================######\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  #with tf.device('/gpu:1'):\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "    tf_train_dataset1 = tf.placeholder(tf.float32,\n",
    "                                  shape=(batch_size_1, input_size))\n",
    "    tf_train_labels1 = tf.placeholder(tf.float32, shape=(batch_size_1, num_labels_1))\n",
    "    tf_train_dataset2 = tf.placeholder(tf.float32,\n",
    "                                  shape=(batch_size_2, input_size))\n",
    "    tf_train_labels2 = tf.placeholder(tf.float32, shape=(batch_size_2, num_labels_2))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset,dtype=tf.float32)\n",
    "    tf_test_dataset = tf.constant(test_dataset,dtype=tf.float32)\n",
    "  \n",
    "  \n",
    "\n",
    "# Variables , here the shared weight being the weight1 .\n",
    "    weights1 = tf.Variable(\n",
    "    tf.truncated_normal([input_size, H1[0]]),name=\"weights1\")\n",
    "    biases1 = tf.Variable(tf.zeros([H1[0]]),name=\"biases1\")\n",
    "    weights2_task1 = tf.Variable(\n",
    "        tf.truncated_normal([H1[0] , H1[1]]),name=\"weights21\")\n",
    "    biases2_task1 = tf.Variable(tf.zeros([H1[1]]),name=\"biases21\")\n",
    "    weights2_task2 = tf.Variable(\n",
    "        tf.truncated_normal([H2[0] , H2[1]]),name=\"weights22\")\n",
    "    biases2_task2 = tf.Variable(tf.zeros([H2[1]]),name=\"biases22\")\n",
    "    weights3_task1 = tf.Variable(\n",
    "        tf.truncated_normal([H1[1] , num_labels_1]),name=\"weights31\")\n",
    "    biases3_task1 = tf.Variable(tf.zeros([num_labels_1]),name=\"biases31\")\n",
    "    weights3_task2 = tf.Variable(\n",
    "        tf.truncated_normal([H2[1] , H2[2]]),name=\"weights32\")\n",
    "    biases3_task2 = tf.Variable(tf.zeros([H2[2]]),name=\"biases32\")\n",
    "    weights4_task2 = tf.Variable(\n",
    "        tf.truncated_normal([H2[2] , num_labels_2]),name=\"weights42\")\n",
    "    biases4_task2 = tf.Variable(tf.zeros([num_labels_2]),name=\"biases42\")\n",
    "    \n",
    "  \n",
    "  \n",
    "# Training computation for the two tasks.\n",
    "    #Task 1\n",
    "    X=tf_train_dataset1\n",
    "    layer2 = tf.matmul(X, weights1) + biases1\n",
    "    layer2out = tf.nn.relu(layer2)\n",
    "    layer2out = tf.nn.dropout(layer2out,0.5)\n",
    "    layer31 = tf.matmul(layer2out, weights2_task1) + biases2_task1\n",
    "    layer31out = tf.nn.relu(layer31)\n",
    "    layer31out = tf.nn.dropout(layer31out,0.5)\n",
    "    layer41 = tf.matmul(layer31out, weights3_task1) + biases3_task1\n",
    "    output1 = tf.nn.softmax(layer41)\n",
    "    \n",
    "    #Task 2\n",
    "    X=tf_train_dataset2\n",
    "    layer2 = tf.matmul(X, weights1) + biases1\n",
    "    layer2out = tf.nn.relu(layer2)\n",
    "    layer2out = tf.nn.dropout(layer2out,0.5)\n",
    "    layer32 = tf.matmul(layer2out, weights2_task2) + biases2_task2\n",
    "    layer32out = tf.nn.relu(layer32)\n",
    "    layer32out = tf.nn.dropout(layer32out,0.5)\n",
    "    layer42 = tf.matmul(layer32out, weights3_task2) + biases3_task2\n",
    "    layer42out = tf.nn.relu(layer42)\n",
    "    layer42out = tf.nn.dropout(layer42out,0.5)\n",
    "    layer52 = tf.matmul(layer42out, weights4_task2) + biases4_task2\n",
    "    output2 = tf.nn.softmax(layer52)\n",
    "\n",
    "    loss1 = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(layer41, tf_train_labels1)) \n",
    "    loss2 = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(layer52, tf_train_labels2))\n",
    "\n",
    "# Optimizers for the two tasks.\n",
    "    optimizer1 = tf.train.AdamOptimizer(learn_rate).minimize(loss1)\n",
    "    optimizer2 = tf.train.AdamOptimizer(learn_rate).minimize(loss2)\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "    train_prediction1 = output1\n",
    "    train_prediction2 = output2\n",
    "    X=tf_valid_dataset\n",
    "    valid_prediction = predictTask1(X,weights1,weights2_task1,weights3_task1,biases1,biases2_task1,biases3_task1)\n",
    "    X=tf_test_dataset\n",
    "    test_prediction = predictTask2(X,weights1,weights2_task2,weights3_task2,weights4_task2,biases1,biases2_task2,\n",
    "                                   biases3_task2,biases4_task2)\n",
    "\n",
    "\n",
    "######==========Running Graphs========================================\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "     #with tf.device(\"/gpu:0\"):\n",
    "        saver=tf.train.Saver(var_list={\"biases1\": biases1, \"weights1\": weights1, \"biases21\": biases2_task1, \n",
    "                                   \"weights21\": weights2_task1 ,\"biases22\": biases2_task2, \n",
    "                                   \"weights22\": weights2_task2 ,\"biases31\": biases3_task1, \n",
    "                                   \"weights31\": weights3_task1 ,\"biases32\": biases3_task2, \n",
    "                                   \"weights32\": weights3_task2 ,\"biases42\": biases4_task2, \n",
    "                                   \"weights42\": weights4_task2 })\n",
    "        if FLAG.randomInit:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print(\"Initialized\")\n",
    "        else:\n",
    "            tf.initialize_all_variables().run()\n",
    "            saver.restore(session,checkPointPath)\n",
    "        step_1=0\n",
    "        step_2=0\n",
    "        for step in range(num_steps):\n",
    "            if step==0 or np.random.rand() < 0.5:\n",
    "            # Pick an offset within the training data 1, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "                offset_1 = (step_1 * batch_size_1) % (train_labels1.shape[0] - batch_size_1)\n",
    "                step_1=step_1+1\n",
    "            # Generate a minibatch1.\n",
    "                batch_data_1 = train_dataset1[offset_1:(offset_1 + batch_size_1), :]\n",
    "                batch_labels_1 = train_labels1[offset_1:(offset_1 + batch_size_1), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch1.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "                feed_dict_1 = {tf_train_dataset1 : batch_data_1, tf_train_labels1 : batch_labels_1}\n",
    "                _, l1, predictions1 = session.run(\n",
    "                [optimizer1, loss1, train_prediction1], feed_dict=feed_dict_1)\n",
    "            if step==0 or np.random.rand() >= 0.5:\n",
    "            # Pick an offset within the training data1, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "                offset_2 = (step_2 * batch_size_2) % (train_labels2.shape[0] - batch_size_2)\n",
    "                step_2=step_2+1\n",
    "            # Generate a minibatch2.\n",
    "                batch_data_2 = train_dataset2[offset_2:(offset_2 + batch_size_2), :]\n",
    "                batch_labels_2 = train_labels2[offset_2:(offset_2 + batch_size_2), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch2.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "                feed_dict_2 = {tf_train_dataset2 : batch_data_2, tf_train_labels2 : batch_labels_2}\n",
    "                _, l2, predictions2 = session.run(\n",
    "                [optimizer2, loss2, train_prediction2], feed_dict=feed_dict_2)\n",
    "           \n",
    "            \n",
    "            if (step % 300 == 0):\n",
    "                print(\"Minibatch loss task1 at step %d: %f\" % (step, l1))\n",
    "                print(\"Minibatch loss task2 at step %d: %f\" % (step, l2))\n",
    "                print(\"Minibatch accuracy task 1: %.1f%%\" % accuracy(predictions1, batch_labels_1))\n",
    "                print(\"Minibatch accuracy task 2: %.1f%%\" % accuracy(predictions2, batch_labels_2))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "                print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "            if(step==num_steps-1):\n",
    "                save_path=saver.save(session,checkPointPath2)\n",
    "        print(session.run(weights1))   \n",
    "        session.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
